{
  "cells": [
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": [
      "# In google colab, \n",
      "# For master version of catalyst, uncomment:\n",
      "# (master version should be fully compatible with this notebook)\n",
      "# ! pip install git+git://github.com/catalyst-team/catalyst.git\n",
      "\n",
      "# For last release version of catalyst, uncomment:\n",
      "# ! pip install catalyst\n",
      "\n",
      "# For specific commit version of catalyst, uncomment:\n",
      "# ! pip install git+http://github.com/catalyst-team/catalyst.git@{commit_hash}"
     ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "bs \u003d 32\n",
        "num_workers \u003d 4\n",
        "\n",
        "data_transform \u003d transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "loaders \u003d collections.OrderedDict()\n",
        "\n",
        "trainset \u003d torchvision.datasets.CIFAR10(\n",
        "    root\u003d\u0027./data\u0027, train\u003dTrue,\n",
        "    download\u003dTrue, transform\u003ddata_transform)\n",
        "trainloader \u003d torch.utils.data.DataLoader(\n",
        "    trainset, batch_size\u003dbs,\n",
        "    shuffle\u003dTrue, num_workers\u003dnum_workers)\n",
        "\n",
        "testset \u003d torchvision.datasets.CIFAR10(\n",
        "    root\u003d\u0027./data\u0027, train\u003dFalse,\n",
        "    download\u003dTrue, transform\u003ddata_transform)\n",
        "testloader \u003d torch.utils.data.DataLoader(\n",
        "    testset, batch_size\u003dbs,\n",
        "    shuffle\u003dFalse, num_workers\u003dnum_workers)\n",
        "\n",
        "loaders[\"train\"] \u003d trainloader\n",
        "loaders[\"valid\"] \u003d testloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 \u003d nn.Conv2d(3, 6, 5)\n",
        "        self.pool \u003d nn.MaxPool2d(2, 2)\n",
        "        self.conv2 \u003d nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 \u003d nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 \u003d nn.Linear(120, 84)\n",
        "        self.fc3 \u003d nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x \u003d self.pool(F.relu(self.conv1(x)))\n",
        "        x \u003d self.pool(F.relu(self.conv2(x)))\n",
        "        x \u003d x.view(-1, 16 * 5 * 5)\n",
        "        x \u003d F.relu(self.fc1(x))\n",
        "        x \u003d F.relu(self.fc2(x))\n",
        "        x \u003d self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Intro\n",
        "@TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# for graphs use `tensorboard --logdir\u003d./logs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.utils import UtilsFactory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 1 - typical training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.experiments import SupervisedRunner\n",
        "\n",
        "# experiment setup\n",
        "num_epochs \u003d 10\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_1\"\n",
        "\n",
        "# model, criterion, optimizer\n",
        "model \u003d Net()\n",
        "criterion \u003d nn.CrossEntropyLoss()\n",
        "optimizer \u003d torch.optim.Adam(model.parameters())\n",
        "\n",
        "# model runner\n",
        "runner \u003d SupervisedRunner()\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    loaders\u003dloaders,\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs,\n",
        "    verbose\u003dTrue\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# you can use plotly and tensorboard to plot metrics inside jupyter\n",
        "# by default it only plots loss\n",
        "UtilsFactory.plot_metrics(logdir\u003dlogdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 2 - training with scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.experiments import SupervisedRunner\n",
        "\n",
        "# experiment setup\n",
        "num_epochs \u003d 10\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_2\"\n",
        "\n",
        "# model, criterion, optimizer\n",
        "model \u003d Net()\n",
        "criterion \u003d nn.CrossEntropyLoss()\n",
        "optimizer \u003d torch.optim.Adam(model.parameters())\n",
        "\n",
        "# any Pytorch scheduler supported\n",
        "# scheduler \u003d torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones\u003d[3, 8], gamma\u003d0.3)\n",
        "scheduler \u003d torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor\u003d0.5, patience\u003d2)\n",
        "\n",
        "# model runner\n",
        "runner \u003d SupervisedRunner()\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    scheduler\u003dscheduler,\n",
        "    loaders\u003dloaders,\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs,\n",
        "    verbose\u003dTrue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 3 - training with early stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.experiments import SupervisedRunner\n",
        "from catalyst.dl.callbacks import EarlyStoppingCallback\n",
        "\n",
        "# experiment setup\n",
        "num_epochs \u003d 10\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_3\"\n",
        "\n",
        "# model, criterion, optimizer, scheduler\n",
        "model \u003d Net()\n",
        "criterion \u003d nn.CrossEntropyLoss()\n",
        "optimizer \u003d torch.optim.Adam(model.parameters())\n",
        "scheduler \u003d torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones\u003d[3, 8], gamma\u003d0.3)\n",
        "\n",
        "# model runner\n",
        "runner \u003d SupervisedRunner()\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    scheduler\u003dscheduler,\n",
        "    loaders\u003dloaders,\n",
        "    callbacks\u003d[\n",
        "        EarlyStoppingCallback(patience\u003d2, min_delta\u003d0.01)\n",
        "    ],\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs,\n",
        "    verbose\u003dTrue\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "UtilsFactory.plot_metrics(logdir\u003dlogdir, metrics\u003d[\"loss\", \"_base/lr\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 4 - training with additional metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.experiments import SupervisedRunner\n",
        "from catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\n",
        "\n",
        "# experiment setup\n",
        "num_epochs \u003d 10\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_4\"\n",
        "\n",
        "# model, criterion, optimizer, scheduler\n",
        "model \u003d Net()\n",
        "criterion \u003d nn.CrossEntropyLoss()\n",
        "optimizer \u003d torch.optim.Adam(model.parameters())\n",
        "scheduler \u003d torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones\u003d[3, 8], gamma\u003d0.3)\n",
        "\n",
        "# model runner\n",
        "runner \u003d SupervisedRunner()\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    scheduler\u003dscheduler,\n",
        "    loaders\u003dloaders,\n",
        "    callbacks\u003d[\n",
        "        AccuracyCallback(accuracy_args\u003d[1, 3, 5]),\n",
        "        EarlyStoppingCallback(patience\u003d2, min_delta\u003d0.01)\n",
        "    ],\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs,\n",
        "    verbose\u003dTrue\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "UtilsFactory.plot_metrics(\n    logdir\u003dlogdir, \n    metrics\u003d[\"loss\", \"accuracy01\", \"accuracy03\", \"_base/lr\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 5 - training with 1cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from catalyst.dl.experiments import SupervisedRunner\nfrom catalyst.dl.callbacks import \\\n    EarlyStoppingCallback, AccuracyCallback\nfrom catalyst.contrib.scheduler import OneCycleLR\n\n# experiment setup\nnum_epochs \u003d 10\nlogdir \u003d \"./logs/cifar_simple_notebook_5\"\n\n# model, criterion, optimizer, scheduler\nmodel \u003d Net()\ncriterion \u003d nn.CrossEntropyLoss()\noptimizer \u003d torch.optim.Adam(model.parameters())\nscheduler \u003d OneCycleLR(\n    optimizer,\n    num_steps\u003dnum_epochs, \n    lr_range\u003d(0.005, 0.00005),\n    warmup_steps\u003d2,\n    momentum_range\u003d(0.85, 0.95)\n)\n\n# model runner\nrunner \u003d SupervisedRunner()\n\n# model training\nrunner.train(\n    model\u003dmodel,\n    criterion\u003dcriterion,\n    optimizer\u003doptimizer,\n    scheduler\u003dscheduler,\n    loaders\u003dloaders,\n    callbacks\u003d[\n        AccuracyCallback(accuracy_args\u003d[1, 3, 5]),\n        EarlyStoppingCallback(patience\u003d2, min_delta\u003d0.01),\n    ],\n    logdir\u003dlogdir,\n    num_epochs\u003dnum_epochs,\n    verbose\u003dTrue\n)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "pycharm": {}
      },
      "outputs": [],
      "source": "UtilsFactory.plot_metrics(\n    logdir\u003dlogdir,\n    step\u003d\"batch\",\n    metrics\u003d[\"loss\", \"accuracy01\", \"_base/lr\", \"_base/momentum\"])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 6 - pipeline check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.experiments import SupervisedRunner\n",
        "\n",
        "# experiment setup\n",
        "num_epochs \u003d 10\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_6\"\n",
        "\n",
        "# model, criterion, optimizer, scheduler\n",
        "model \u003d Net()\n",
        "criterion \u003d nn.CrossEntropyLoss()\n",
        "optimizer \u003d torch.optim.Adam(model.parameters())\n",
        "scheduler \u003d torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones\u003d[3, 8], gamma\u003d0.3)\n",
        "\n",
        "# model runner\n",
        "runner \u003d SupervisedRunner()\n",
        "\n",
        "# model training\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    scheduler\u003dscheduler,\n",
        "    loaders\u003dloaders,\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs,\n",
        "    verbose\u003dTrue,\n",
        "    check\u003dTrue  # here is the trick\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 7 - multi-stage training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.experiments import SupervisedRunner\n",
        "from catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\n",
        "\n",
        "# experiment setup\n",
        "num_epochs \u003d 10\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_7\"\n",
        "\n",
        "# model, criterion, optimizer, scheduler\n",
        "model \u003d Net()\n",
        "criterion \u003d nn.CrossEntropyLoss()\n",
        "optimizer \u003d torch.optim.Adam(model.parameters())\n",
        "scheduler \u003d torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones\u003d[3, 8], gamma\u003d0.3)\n",
        "\n",
        "# model runner\n",
        "runner \u003d SupervisedRunner()\n",
        "\n",
        "# model training - 1\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    scheduler\u003dscheduler,\n",
        "    loaders\u003dloaders,\n",
        "    callbacks\u003d[\n",
        "        AccuracyCallback(accuracy_args\u003d[1, 3, 5]),\n",
        "        EarlyStoppingCallback(patience\u003d2, min_delta\u003d0.01)\n",
        "    ],\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs,\n",
        "    verbose\u003dTrue\n",
        ")\n",
        "\n",
        "# model training - 2\n",
        "num_epochs \u003d 5\n",
        "logdir \u003d \"./logs/cifar_simple_notebook_8\"\n",
        "optimizer \u003d torch.optim.SGD(model.parameters(), lr\u003d0.05)\n",
        "\n",
        "runner.train(\n",
        "    model\u003dmodel,\n",
        "    criterion\u003dcriterion,\n",
        "    optimizer\u003doptimizer,\n",
        "    loaders\u003dloaders,\n",
        "    logdir\u003dlogdir,\n",
        "    num_epochs\u003dnum_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 8 - loader inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from catalyst.dl.callbacks import InferCallback\n",
        "loaders \u003d collections.OrderedDict([(\"infer\", loaders[\"train\"])])\n",
        "runner.infer(\n",
        "    model\u003dmodel,\n",
        "    loaders\u003dloaders,\n",
        "    callbacks\u003d[InferCallback()],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "runner.callbacks[0].predictions[\"logits\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Setup 9 - batch inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "features, targets \u003d next(iter(loaders[\"infer\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "runner_in \u003d {runner.input_key: features}\n",
        "runner_out \u003d runner.predict_batch(runner_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "runner_out[\"logits\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}