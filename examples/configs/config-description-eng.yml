# At this level, any additional parameters in addition to the required keywords can be found.
shared:  # Example
  key: &key value
  key2: &key2 value2

args:  # REQUIRED KEYWORD, various arguments for Catalyst
  expdir: "src"  # REQUIRED KEYWORD, the path to your experiment, with the file `__init__` in which you import Experiment, Runner, and optionally all additional entities are registered: model, callback, criterion, etc
  logdir: /path/to/logdir  # KEYWORD, the path to which the logs will be saved (can be empty if `baselogdir` is passed)
  baselogdir: /path/to/baselogdir  # KEYWORD optional argument -- path for the root with logs (if it is specified but not logdir, then the logdir will be generated as `{baselogdir}/{ConfigExperiment._get_logdir(config)}`)
  seed: 42  # KEYWORD, training seed for PyTorch, Numpy, Python and Tensorflow (Default is 42)
  deterministic: True  # KEYWORD, whether to use deterministic CuDNN (Default is True)
  benchmark: True  # KEYWORD, whether to use CuDNN benchmark
  verbose: False  # KEYWORD, whether to display learning information on the console (Default is False)
  check: False  # KEYWORD, if True, then Catalyst does only 3 epochs (to check the performance of the pipeline, by default False)

runner:  # REQUIRED KEYWORD, runner parameters
  # For example for SupervisedRunner
  _target_: SupervisedRunner  # REQUIRED KEYWORD, class name. The class itself will be constructed in the registry by this name.
  # At this level, the parameters for __init__ of the runner can be found, for example
  input_key: "features"  # Example
  output_key: null  # Example
  # for more initialization parameters please refer to the target object docs.

engine:  # REQUIRED KEYWORD, engine parameters
  _target_: DeviceEngine  # REQUIRED KEYWORD, class name. The class itself will be constructed in the registry by this name.
  # At this level, the parameters for __init__ of the engine can be found, for example
  device: "cpu" # Example
  # for more initialization parameters please refer to the target object docs.

model:  # REQUIRED KEYWORD, model(s) parameters
  _key_value: False  # OPTIONAL KEYWORD, if True, there may be several models and then they need to be wrapped in key-value.
  _target_: ModelName  # REQUIRED KEYWORD, class name. The class itself will be constructed in the registry by this name.
  # At this level, the parameters for __init__ of the model can be found, for example
  arch: resnet18  # with this key, the model will be created as follows: `ModelName(arch=resnet18)`

loggers:  # KEYWORD, loggers parameters
  console:
    _target_: ConsoleLogger  # REQUIRED KEYWORD, class name. The class itself will be constructed in the registry by this name.
    # At this level, the parameters for __init__ of the logger can be found
    # for more initialization parameters please refer to the target object docs.
  tensorboard:
    _target_: TensorboardLogger

stages:  # REQUIRED KEYWORD, dictionary of all stages of Catalyst, for training and/or infer. Contain keywords with parameters that apply to all the stages, as well as the names of the stages

  stage1:  # Anything that's not a keyword is considered a name for a stage. For training in Catalyst, at least one stage is required. The name can be anything.
    num_epochs: 10  # KEYWORD, The number of epochs in this stage

    loaders: &loaders # REQUIRED KEYWORD
      # kwargs for `runner.get_loaders(...)` for this stage
      batch_size: 1  # KEYWORD, batch size for all the stages
      num_workers: 1  # KEYWORD, Number of parallel processes for DataLoader
      drop_last: False  # KEYWORD, parameter for DataLoader (Default is False)
      loaders_params:  # KEYWORD, dict with additional loaders parameters. Also can do it for Sampler by dict with name samplers_params
        persistent_workers: True # KEYWORD, parameter example. Full list of parameters you can find in torch.utils.data.DataLoader documentation
      per_gpu_scaling: False  # KEYWORD, if True it increases the batch size and the number of workers in proportion to the number of GPUs (for distributed increases only batch_size)

      # kwargs for `runner.get_datasets(...)` for this stage
      some_extra_key: "some_extra_value"

    criterion: # REQUIRED KEYWORD, parameters for the loss function
      _key_value: False  # OPTIONAL KEYWORD, if True, there may be several loss-functions and then they should be wrapped in key-value

      _target_: BCEWithLogitsLoss  # REQUIRED KEYWORD, name of the loss function
      # At this level, the parameters for __init__ of the loss function can be found, for example
      reduction: sum
      # for more initialization parameters please refer to the target object docs.

    optimizer: # REQUIRED KEYWORD, parameters for the optimizer
      _key_value: False  # OPTIONAL KEYWORD, if True, there may be several optimizers and then they should be wrapped in key-value
      layerwise_params: # OPTIONAL KEYWORD, optimizer parameters for different network layers, optional
        conv1.*: # regexp with layer name
          lr: 0.001
          weight_decay: 0.0003
        encoder.conv.*:
          lr: 0.002
      no_bias_weight_decay: True  # OPTIONAL KEYWORD whether to remove weight decay from the all bias parameters of the network (Default is True)
      lr_linear_scaling: # OPTIONAL KEYWORD, parameters for linear lr scaling
        lr: 0.001
        base_batch_size: 64  # KEYWORD, size of the base batch size before scaling

      _target_: Adam  # REQUIRED KEYWORD, name of the optimizer
      # At this level, the parameters for __init__ of the optimizer can be found, for example
      lr: 0.003
      weight_decay: 0.0001
      # for more initialization parameters please refer to the target object docs.

    scheduler: # REQUIRED KEYWORD, params for lr-scheduler
      _key_value: False  # OPTIONAL KEYWORD, if True, there may be several lr-schedulers and then they should be wrapped in key-value

      _target_: StepLR  # REQUIRED KEYWORD, name of the lr-scheduler
      # At this level, the parameters for __init__ of the lr-scheduler can be found, for example
      step_size: 10
      gamma: 0.3
      # for more initialization parameters please refer to the target object docs.

    callbacks:  &callbacks # REQUIRED KEYWORD, The most important part. It's where all the callbacks are written down for this stage.
    # Callbacks are written through key-value
      loss:
        _target_: CriterionCallback  # REQUIRED KEYWORD name of the callback
      optimizer:
        _target_: OptimizerCallback
      scheduler:
        _target_: SchedulerCallback
        # At this level, the parameters for any callback can be found, for example
        loader_key: "valid"
        metric_key: "loss"
        # for more initialization parameters please refer to the target object docs.
      saver:
        _target_: CheckpointCallback
        save_n_best: 3

  finetune:  # Example of a second training stage, here we can change our parameters
    num_epochs: 3

    loaders: *loaders  # Example of passing the loaders to the next stage

    criterion: # Example of an overridden criterion
      _target_: CrossEntropyLoss

    optimizer: # Example of an overridden optimizer
      _target_: Ralamb

    scheduler:
      _target_: MultiStepLR
      milestones: [ 2 ]
      gamma: 0.3

    callbacks: *callbacks  # Example of passing the callbacks to the next stage

  # the number of stages can be as many as you like.
