# Refer to configs/config-description-eng.yml
# for detailed comments on this configuration file

study_params:
  pruner_params:
    pruner: MedianPruner
    n_startup_trials: 1
    n_warmup_steps: 0
    interval_steps: 1


model_params:
  _target_: SimpleNet
  num_filters1: "int(trial.suggest_loguniform('num_filters1', 4, 32))"
  num_filters2: "int(trial.suggest_loguniform('num_filters2', 4, 32))"
  num_hiddens1: "int(trial.suggest_loguniform('num_hiddens1', 32, 128))"
  num_hiddens2: "int(trial.suggest_loguniform('num_hiddens2', 32, 128))"


runner_params:
  runner: SupervisedRunner
  input_key: image


args:
  expdir: "cifar_stages_optuna"
  baselogdir: "./logs/cifar_stages_optuna"
#  n_jobs: 2


stages:

  data_params:
    batch_size: 64
    num_workers: 1

  transform_params:
    _target_: albumentations.Compose
    transforms:
      - _target_: albumentations.Normalize
      - _target_: catalyst.ImageToTensor

  stage_params:
    num_epochs: 3
    main_metric: &reduced_metric accuracy01
    minimize_metric: False

  criterion_params:
    _target_: CrossEntropyLoss

  scheduler_params:
    _target_: MultiStepLR
    milestones: [10]
    gamma: 0.3

  callbacks_params:
    loss:
      _target_: CriterionCallback
    optimizer:
      _target_: OptimizerCallback
    accuracy:
      _target_: AccuracyCallback
      accuracy_args: [1, 3, 5]
    scheduler:
      _target_: SchedulerCallback
      reduced_metric: *reduced_metric
    saver:
      _target_: CheckpointCallback
    optuna_pruner:
      _target_: OptunaPruningCallback

  stage1:
    optimizer_params:
      _target_: Ralamb

      layerwise_params:
        conv1.*:
          lr: 0.001
          weight_decay: 0.0003
        conv.*:
          lr: 0.002

      lr_linear_scaling:
        lr: 0.001
        base_batch_size: 64
      weight_decay: 0.0001
      no_bias_weight_decay: True # removes `weight_decay` for model's biases

  # tune
  stage2:
    stage_params:
      num_epochs: 2

    optimizer_params:
      load_from_previous_stage: True
      _target_: Ralamb
      lr_linear_scaling:
        lr: 0.0001
        base_batch_size: 64
      weight_decay: 0.0001
      no_bias_weight_decay: False
