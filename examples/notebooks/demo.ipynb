{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "Minimal working examples with [Catalyst](https://github.com/catalyst-team/catalyst).\n",
    "- CV - mnist classification\n",
    "- NLP - sentiment analysis\n",
    "- RecSys - movie recommendations\n",
    "- GAN - mnist again :)\n",
    "\n",
    "Comparison with pure [PyTorch](https://github.com/catalyst-team/catalyst) code included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U torch==1.4.0 torchvision==0.5.0 torchtext==0.5.0 catalyst==20.02.4 pandas==1.0.1 tqdm==4.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tensorboard integration\n",
    "# !pip install tensorflow\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True, 'batch_size': 100}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data/cv', train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])),\n",
    "    shuffle=True, **kwargs)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data/cv', train=False, \n",
    "        transform=transforms.Compose([\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize((0.1307,), (0.3081,))\n",
    "       ])),\n",
    "    shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "model = Net()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, device, loader, optimizer, epoch, is_train=True):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss_, accuracy_ = 0, 0\n",
    "    prefix = 'Train epoch' if is_train else 'Valid epoch'\n",
    "    \n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            # dataflow\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "\n",
    "            # metrics\n",
    "            loss_ += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            accuracy_ += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # optimization\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # logging\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    '{} {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    prefix, epoch, batch_idx * len(data), len(loader.dataset),\n",
    "                    100. * batch_idx / len(loader), loss.item()))\n",
    "\n",
    "        loss_ /= len(loader.dataset)\n",
    "        print('\\{} {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        prefix, epoch, loss_, accuracy_, len(loader.dataset),\n",
    "        100. * accuracy_ / len(loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    run_epoch(model, device, train_loader, optimizer, epoch, is_train=True)\n",
    "    run_epoch(model, device, valid_loader, None, epoch, is_train=False)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner, \\\n",
    "    AccuracyCallback\n",
    "\n",
    "runner = SupervisedRunner()\n",
    "runner.train(\n",
    "    model=model, \n",
    "    criterion=nn.NLLLoss(), # a bit different loss compute\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    loaders={'train': train_loader, 'valid': valid_loader},\n",
    "    logdir=\"./logs/cv\",\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    callbacks=[AccuracyCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "if not os.path.isdir('./data/nlp'):\n",
    "    os.mkdir('./data/nlp')\n",
    "train_dataset, valid_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data/nlp', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(train_dataset.get_labels())\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    output = {\n",
    "        \"text\": text,\n",
    "        \"offsets\": offsets,\n",
    "        \"label\": label\n",
    "    }\n",
    "    return output\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=generate_batch,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    collate_fn=generate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, model, device, criterion, optimizer, scheduler, epoch, is_train=True):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss_, accuracy_ = 0, 0\n",
    "    prefix = 'Train epoch' if is_train else 'Valid epoch'\n",
    "    \n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        text, offsets, cls = batch[\"text\"], batch[\"offsets\"], batch[\"label\"]\n",
    "        # dataflow\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        # metrics\n",
    "        loss = criterion(output, cls)\n",
    "        loss_ += loss.item()\n",
    "        accuracy_ += (output.argmax(1) == cls).sum().item()\n",
    "        \n",
    "        # optimization\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # logging\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(\n",
    "                '{} {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                prefix, epoch, batch_idx * len(text), len(loader.dataset),\n",
    "                100. * batch_idx / len(loader), loss.item()))\n",
    "\n",
    "    if is_train:\n",
    "        # Adjust the learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    loss_ /= len(loader)\n",
    "    accuracy_ /= len(loader)\n",
    "\n",
    "    print('\\{} {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        prefix, epoch, loss_, accuracy_, len(loader.dataset),\n",
    "        100. * accuracy_ / len(loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    run_epoch(train_loader, model, device, criterion, optimizer, scheduler, epoch, is_train=True)\n",
    "    run_epoch(valid_loader, model, device, criterion, None, None, epoch, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner, \\\n",
    "    CriterionCallback, AccuracyCallback\n",
    "\n",
    "# input_keys - which key from dataloader we need to pass to the model\n",
    "runner = SupervisedRunner(input_key=[\"text\", \"offsets\"])\n",
    "\n",
    "runner.train(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler,\n",
    "    loaders={'train': train_loader, 'valid': valid_loader},\n",
    "    logdir=\"./logs/nlp\",\n",
    "    num_epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    # input_key - which key from dataloader we need to pass to criterion as target label\n",
    "    callbacks=[\n",
    "        CriterionCallback(input_key=\"label\"),\n",
    "        AccuracyCallback(input_key=\"label\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecSys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.utils.data as td\n",
    "import torch.optim as to\n",
    "\n",
    "import matplotlib.pyplot as pl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# The directory to store the data\n",
    "data_dir = \"data/recsys\"\n",
    "\n",
    "train_rating = \"ml-1m.train.rating\"\n",
    "test_negative = \"ml-1m.test.negative\"\n",
    "\n",
    "# NCF config\n",
    "train_negative_samples = 4\n",
    "test_negative_samples = 99\n",
    "embedding_dim = 64\n",
    "hidden_dim = 32\n",
    "\n",
    "# Training config\n",
    "batch_size = 256\n",
    "epochs = 10  # Original implementation uses 20\n",
    "top_k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "\n",
    "Use Movielens 1M data from the NCF paper authors' implementation https://github.com/hexiangnan/neural_collaborative_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "if not os.path.isdir('./data/recsys'):\n",
    "    os.mkdir('./data/recsys')\n",
    "    \n",
    "for file_name in [train_rating, test_negative]:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"Skip loading \" + file_name)\n",
    "        continue\n",
    "    with open(file_path, \"wb\") as tf:\n",
    "        print(\"Load \" + file_name)\n",
    "        r = requests.get(\"https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/\" + file_name, allow_redirects=True)\n",
    "        tf.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train():\n",
    "    train_data = pd.read_csv(os.path.join(data_dir, train_rating), sep='\\t', header=None, names=['user', 'item'], usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
    "\n",
    "    user_num = train_data['user'].max() + 1\n",
    "    item_num = train_data['item'].max() + 1\n",
    "\n",
    "    train_data = train_data.values.tolist()\n",
    "\n",
    "    # Convert ratings as a dok matrix\n",
    "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
    "    for user, item in train_data:\n",
    "        train_mat[user, item] = 1.0\n",
    "        \n",
    "    return train_data, train_mat, user_num, item_num\n",
    "\n",
    "\n",
    "train_data, train_mat, user_num, item_num = preprocess_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test():\n",
    "    test_data = []\n",
    "    with open(os.path.join(data_dir, test_negative)) as tnf:\n",
    "        for line in tnf:\n",
    "            parts = line.split('\\t')\n",
    "            assert len(parts) == test_negative_samples + 1\n",
    "            \n",
    "            user, positive = eval(parts[0])\n",
    "            test_data.append([user, positive])\n",
    "            \n",
    "            for negative in parts[1:]:\n",
    "                test_data.append([user, int(negative)])\n",
    "\n",
    "    return test_data\n",
    "\n",
    "\n",
    "valid_data = preprocess_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCFDataset(td.Dataset):\n",
    "    \n",
    "    def __init__(self, positive_data, item_num, positive_mat, negative_samples=0):\n",
    "        super(NCFDataset, self).__init__()\n",
    "        self.positive_data = positive_data\n",
    "        self.item_num = item_num\n",
    "        self.positive_mat = positive_mat\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        print(\"Resetting dataset\")\n",
    "        if self.negative_samples > 0:\n",
    "            negative_data = self.sample_negatives()\n",
    "            data = self.positive_data + negative_data\n",
    "            labels = [1] * len(self.positive_data) + [0] * len(negative_data)\n",
    "        else:\n",
    "            data = self.positive_data\n",
    "            labels = [0] * len(self.positive_data)\n",
    "            \n",
    "        self.data = np.concatenate([\n",
    "            np.array(data), \n",
    "            np.array(labels)[:, np.newaxis]], \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "\n",
    "    def sample_negatives(self):\n",
    "        negative_data = []\n",
    "        for user, positive in self.positive_data:\n",
    "            for _ in range(self.negative_samples):\n",
    "                negative = np.random.randint(self.item_num)\n",
    "                while (user, negative) in self.positive_mat:\n",
    "                    negative = np.random.randint(self.item_num)\n",
    "                    \n",
    "                negative_data.append([user, negative])\n",
    "\n",
    "        return negative_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, item, label = self.data[idx]\n",
    "        output = {\n",
    "            \"user\": user,\n",
    "            \"item\": item,\n",
    "            \"label\": np.float32(label),\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    \n",
    "class SamplerWithReset(td.RandomSampler):\n",
    "    def __iter__(self):\n",
    "        self.data_source.reset()\n",
    "        return super().__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NCFDataset(\n",
    "    train_data, \n",
    "    item_num, \n",
    "    train_mat, \n",
    "    train_negative_samples\n",
    ")\n",
    "train_loader = td.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    sampler=SamplerWithReset(train_dataset)\n",
    ")\n",
    "\n",
    "valid_dataset = NCFDataset(valid_data, item_num, train_mat)\n",
    "valid_loader = td.DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=test_negative_samples+1, \n",
    "    shuffle=False, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ncf(nn.Module):\n",
    "    \n",
    "    def __init__(self, user_num, item_num, embedding_dim, hidden_dim):\n",
    "        super(Ncf, self).__init__()\n",
    "        \n",
    "        self.user_embeddings = nn.Embedding(user_num, embedding_dim)\n",
    "        self.item_embeddings = nn.Embedding(item_num, embedding_dim)\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                layer.bias.data.zero_()\n",
    "            \n",
    "    def forward(self, user, item):\n",
    "        user_embedding = self.user_embeddings(user)\n",
    "        item_embedding = self.item_embeddings(item)\n",
    "        concat = torch.cat((user_embedding, item_embedding), -1)\n",
    "        return self.layers(concat).view(-1)\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Ncf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_metric(recommended, actual):\n",
    "    return int(actual in recommended)\n",
    "\n",
    "\n",
    "def dcg_metric(recommended, actual):\n",
    "    if actual in recommended:\n",
    "        index = recommended.index(actual)\n",
    "        return np.reciprocal(np.log2(index + 2))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ncf(user_num, item_num, embedding_dim, hidden_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = to.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(loader, model, device, top_k):\n",
    "    hits, dcgs = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        user, item, label = batch[\"user\"], batch[\"item\"], batch[\"label\"]\n",
    "        item = item.to(device)\n",
    "        \n",
    "        predictions = model(user.to(device), item)\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommended = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        item = item[0].item()\n",
    "        hits.append(hit_metric(recommended, item))\n",
    "        dcgs.append(dcg_metric(recommended, item))\n",
    "\n",
    "    return np.mean(hits), np.mean(dcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs=3):\n",
    "    history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            user, item, label = batch[\"user\"], batch[\"item\"], batch[\"label\"]\n",
    "            model.zero_grad()\n",
    "            prediction = model(user.to(device), item.to(device))\n",
    "            loss = criterion(prediction, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        hr, dcg = metrics(valid_loader, model, device, top_k)\n",
    "        elapsed = time.time() - start_time\n",
    "        history.append({\"model\": model.name(), \"epoch\": epoch, \"hit_rate\": hr, \"dcg\": dcg, \"elapsed\": elapsed})\n",
    "\n",
    "        print(\"[{model}] epoch: {epoch}, hit rate: {hit_rate}, dcg: {dcg}\".format(**history[-1]))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "ncf_history = train(model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import Callback, CallbackOrder, State\n",
    "\n",
    "class NdcgLoaderMetricCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(CallbackOrder.Metric)\n",
    "\n",
    "    def on_batch_end(self, state: State):\n",
    "        item = state.input[\"item\"]\n",
    "        predictions = state.output[\"logits\"]\n",
    "\n",
    "        _, indices = torch.topk(predictions, top_k)\n",
    "        recommended = torch.take(item, indices).cpu().numpy().tolist()\n",
    "\n",
    "        item = item[0].item()\n",
    "\n",
    "        state.metric_manager.add_batch_value(\n",
    "            name=\"hits\", value=hit_metric(recommended, item))\n",
    "        state.metric_manager.add_batch_value(\n",
    "            name=\"dcgs\", value=dcg_metric(recommended, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner, CriterionCallback\n",
    "\n",
    "# input_keys - which key from dataloader we need to pass to the model\n",
    "runner = SupervisedRunner(input_key=[\"user\", \"item\"])\n",
    "\n",
    "runner.train(\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    loaders={'train': train_loader, 'valid': valid_loader},\n",
    "    logdir=\"./logs/recsys\",\n",
    "    num_epochs=3,\n",
    "    verbose=True,\n",
    "    # input_key - which key from dataloader we need to pass to criterion as target label\n",
    "    callbacks=[\n",
    "        CriterionCallback(input_key=\"label\"),\n",
    "        NdcgLoaderMetricCallback()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "dataset = MNIST(\n",
    "    os.getcwd(), train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_shape = (1, 28, 28)\n",
    "latent_dim = 128\n",
    "\n",
    "generator = Generator(latent_dim=latent_dim, img_shape=mnist_shape)\n",
    "discriminator = Discriminator(img_shape=mnist_shape)\n",
    "\n",
    "model = {\n",
    "    \"generator\": generator,\n",
    "    \"discriminator\": discriminator,\n",
    "}\n",
    "model = nn.ModuleDict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "generator_optimizer = torch.optim.Adam(\n",
    "    generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "discriminator_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "optimizer = {\n",
    "    \"generator\": generator_optimizer,\n",
    "    \"discriminator\": discriminator_optimizer,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was too lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import OptimizerCallback, SupervisedRunner\n",
    "\n",
    "\n",
    "class GanRunner(SupervisedRunner):\n",
    "    \n",
    "    def forward(self, batch, **kwargs):\n",
    "        # @TODO add images generation from noise\n",
    "        pass\n",
    "    \n",
    "    def _run_batch_train_step(self, batch):\n",
    "        state = self.state\n",
    "        state.loss = {}\n",
    "        \n",
    "        images = batch[self.input_key]\n",
    "        bs = images.shape[0]\n",
    "        z = torch.randn(bs, latent_dim).to(self.device)\n",
    "        generated_images = self.model[\"generator\"](z)\n",
    "        \n",
    "        # generator step\n",
    "        ## predictions & labels\n",
    "        generated_labels = torch.ones(bs, 1).to(self.device)\n",
    "        generated_pred = self.model[\"discriminator\"](generated_images)\n",
    "\n",
    "        ## loss\n",
    "        loss_generator = F.binary_cross_entropy(generated_pred, generated_labels)\n",
    "        state.loss[\"loss_generator\"] = loss_generator\n",
    "        state.metric_manager.add_batch_value(\"loss_generator\", loss_generator)\n",
    "        \n",
    "        \n",
    "        # discriminator step\n",
    "        ## real\n",
    "        images_labels = torch.ones(bs, 1).to(self.device)\n",
    "        images_pred = self.model[\"discriminator\"](images)\n",
    "        real_loss = F.binary_cross_entropy(images_pred, images_labels)\n",
    "\n",
    "        ## fake\n",
    "        generated_labels_ = torch.zeros(bs, 1).to(self.device)\n",
    "        generated_pred_ = self.model[\"discriminator\"](generated_images.detach())\n",
    "        fake_loss = F.binary_cross_entropy(generated_pred_, generated_labels_)\n",
    "\n",
    "        ## loss\n",
    "        loss_discriminator = (real_loss + fake_loss) / 2.0\n",
    "        state.loss[\"loss_discriminator\"] = loss_discriminator\n",
    "        state.metric_manager.add_batch_value(\"loss_discriminator\", loss_discriminator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = GanRunner()\n",
    "\n",
    "runner.train(\n",
    "    model=model, \n",
    "    optimizer=optimizer,\n",
    "    criterion=None,\n",
    "    loaders={\"train\": loader},\n",
    "    callbacks=[\n",
    "        OptimizerCallback(\n",
    "            optimizer_key=\"generator\", \n",
    "            loss_key=\"loss_generator\"\n",
    "        ),\n",
    "        OptimizerCallback(\n",
    "            optimizer_key=\"discriminator\", \n",
    "            loss_key=\"loss_discriminator\"\n",
    "        ),\n",
    "    ],\n",
    "    main_metric=\"loss_generator\",\n",
    "    num_epochs=5,\n",
    "    logdir=\"./logs/gan\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Resume\n",
    "\n",
    "[__PyTorch__](https://github.com/catalyst-team/catalyst) is great, but too low-level framework,\n",
    "- you need to write lots of code\n",
    "- lack of model saving/selection, visualization tools integration or any Deep Learning best practices like gradient accumulation, fp16 support etc\n",
    "- no full reproducibility\n",
    "\n",
    "[__Catalyst__](https://github.com/catalyst-team/catalyst) - modular framework on top of PyTorch, \n",
    "- easily extendable for your needs\n",
    "- allows you to write less boilerplate\n",
    "- supports many monitoring tools like TensorBoard or Alchemy\n",
    "- integrates lofs of Deep Learning best practices, distributed training, jit.tracing support and many more\n",
    "- framework-wise determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
