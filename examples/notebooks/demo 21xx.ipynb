{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/catalyst-team/catalyst@kittylyst scikit-learn>=0.20 optuna --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install catalyst==21.02rc1 scikit-learn>=0.20 optuna --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Catalyst 21.xx demo\n",
    "## Stage 1: Customization is all u need\n",
    "- 10 minimal examples with different Catalyst customization usecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from catalyst import dl, metrics, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "def make_dataset(seed=42, n_samples=int(1e3)):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    X, y = make_moons(n_samples=n_samples, noise=0.1)\n",
    "\n",
    "    y = y*2 - 1 # make y be -1 or 1\n",
    "    return X, y\n",
    "\n",
    "def visualize_dataset(X, y):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
    "\n",
    "# let's create train data\n",
    "X_train, y_train = make_dataset()\n",
    "visualize_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid data\n",
    "X_valid, y_valid = make_dataset(seed=137)\n",
    "visualize_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another train one (why not?)\n",
    "X_train2, y_train2 = make_dataset(seed=1337)\n",
    "visualize_dataset(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model \n",
    "# 2-layer neural network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 16), nn.ReLU(), \n",
    "    nn.Linear(16, 16), nn.ReLU(), \n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "print(model)\n",
    "# print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(X, y, model):\n",
    "    h = 0.25\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    inputs = torch.tensor([list(xrow) for xrow in Xmesh]).float()\n",
    "    scores = model(inputs)\n",
    "    \n",
    "    Z = np.array([s.data > 0 for s in scores])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train > 0).float())\n",
    "t2 = TensorDataset(torch.tensor(X_train2).float(), torch.tensor(y_train2 > 0).float())\n",
    "v1 = TensorDataset(torch.tensor(X_valid).float(), torch.tensor(y_valid > 0).float())\n",
    "\n",
    "loaders = {\n",
    "    \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "    \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "    \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Act 1 - ``CustomRunner – batch handling by you own``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return None\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return None\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.view(-1), y)\n",
    "        self.batch_metrics = {\"loss\": loss}\n",
    "        if self.loader_batch_step % 10 == 0:\n",
    "            print(\n",
    "                f\"{self.loader_key} ({self.loader_batch_step}/{self.loader_batch_len}:\" \n",
    "                f\"loss {loss.item()}\"\n",
    "            )\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "runner = CustomRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Act 2 - ``SupervisedRunner – Runner with Callbacks``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "    #         \"csv\": dl.LogdirLogger(logdir=\"./logdir02\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir02/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        return {\n",
    "            # Let's use AUC metric as an example – it's loader-based, so we shouldn't compute it on each batch\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            # To wrap the criterion step logic, you could use CriterionCallback:\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            # To wrap the optimizer step logic, you could use OptimizerCallback:\n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            # The same case with the scheduler:\n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            # We could also use lrfinder for lr scheduling:\n",
    "#             \"lr-finder\": dl.LRFinder(\n",
    "#                 final_lr=1.0,\n",
    "#                 scale=\"log\",\n",
    "#                 num_steps=None,\n",
    "#                 optimizer_key=None,\n",
    "#             ),\n",
    "            # You can select any number of metrics to checkpoint on:\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir02/auc\",\n",
    "                loader_key=\"valid\", metric_key=\"auc\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir02/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            # Or turn on/off tqdm verbose during loader run:\n",
    "            \"verbose\": dl.TqdmCallback(),\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 3 - ``CustomMetric``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAccuracyMetric(metrics.ICallbackBatchMetric, metrics.AdditiveValueMetric):\n",
    "    def update(self, scores: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        value = ((scores > 0.5) == targets).float().mean().item()\n",
    "        value = super().update(value, len(targets))\n",
    "        return value\n",
    "    \n",
    "    def update_key_value(self, scores: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:\n",
    "        value = self.update(scores, targets)\n",
    "        return {\"accuracy\": value}\n",
    "\n",
    "    def compute_key_value(self) -> Dict[str, float]:\n",
    "        mean, std = super().compute()\n",
    "        return {\"accuracy\": mean, \"accuracy/std\": std}\n",
    "\n",
    "    \n",
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir03/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        return {\n",
    "            \"accuracy\": dl.BatchMetricCallback(\n",
    "                metric=CustomAccuracyMetric(), log_on_batch=True,\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ),\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir03/accuracy\",\n",
    "                loader_key=\"valid\", metric_key=\"accuracy\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir03/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "    #         \"verbose\": dl.TqdmCallback(),\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 4 - ``CustomCallback``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the decision doundary after each epoch:\n",
    "class VisualizationCallback(dl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(order=dl.CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        img = visualize_decision_boundary(X_valid, y_valid, runner.model)\n",
    "\n",
    "\n",
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir04/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        return {\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir04/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            # And include it into callbacks:        \n",
    "            \"visualization\": VisualizationCallback()\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 5 - ``CustomLogger``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_img_from_fig(fig, dpi=180):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add only a few lines to log the image to all runner's loggers\n",
    "class VisualizationCallback(dl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(order=dl.CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        image = visualize_decision_boundary(X_valid, y_valid, runner.model)\n",
    "        image = get_img_from_fig(image)\n",
    "        # runner will propagate it to all loggers\n",
    "        runner.log_image(tag=\"decision_boundary\", image=image, scope=\"epoch\")\n",
    "\n",
    "\n",
    "# Let's also add our own Logger to store image on the disk\n",
    "class VisualizationLogger(dl.ILogger):\n",
    "    def __init__(self, logdir: str):\n",
    "        self.logdir = logdir\n",
    "        os.makedirs(self.logdir, exist_ok=True)\n",
    "        \n",
    "    def log_image(\n",
    "        self,\n",
    "        tag: str,\n",
    "        image: np.ndarray,\n",
    "        scope: str = None,\n",
    "        # experiment info\n",
    "        experiment_key: str = None,\n",
    "        global_epoch_step: int = 0,\n",
    "        global_batch_step: int = 0,\n",
    "        global_sample_step: int = 0,\n",
    "        # stage info\n",
    "        stage_key: str = None,\n",
    "        stage_epoch_len: int = 0,\n",
    "        stage_epoch_step: int = 0,\n",
    "        stage_batch_step: int = 0,\n",
    "        stage_sample_step: int = 0,\n",
    "        # loader info\n",
    "        loader_key: str = None,\n",
    "        loader_batch_len: int = 0,\n",
    "        loader_sample_len: int = 0,\n",
    "        loader_batch_step: int = 0,\n",
    "        loader_sample_step: int = 0,\n",
    "    ) -> None:\n",
    "        if scope == \"epoch\":\n",
    "            plt.imsave(\n",
    "                os.path.join(self.logdir, f\"{tag}_{stage_key}_{stage_epoch_step}.png\"),\n",
    "                image,\n",
    "            )\n",
    "\n",
    "\n",
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"visualization\": VisualizationLogger(logdir=\"./logdir05/visualization\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir05/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        return {\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir05/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"visualization\": VisualizationCallback()\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./logdir05\n",
    "! ls ./logdir05/loss\n",
    "! ls ./logdir05/tb\n",
    "! ls ./logdir05/visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 6 - ``Multistage Run``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_img_from_fig(fig, dpi=180):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    \"stage_1\": {\n",
    "        \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "        \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "    },\n",
    "    \"stage_2\": {\n",
    "        \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "        \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "    },\n",
    "}\n",
    "\n",
    "    \n",
    "class VisualizationCallback(dl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(order=dl.CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        image = visualize_decision_boundary(X_valid, y_valid, runner.model)\n",
    "        image = get_img_from_fig(image)\n",
    "        # runner will propagate it to all loggers\n",
    "        runner.log_image(tag=\"decision_boundary\", image=image, scope=\"epoch\")\n",
    "\n",
    "\n",
    "class VisualizationLogger(dl.ILogger):\n",
    "    def __init__(self, logdir: str):\n",
    "        self.logdir = logdir\n",
    "        os.makedirs(self.logdir, exist_ok=True)\n",
    "        \n",
    "    def log_image(\n",
    "        self,\n",
    "        tag: str,\n",
    "        image: np.ndarray,\n",
    "        scope: str = None,\n",
    "        # experiment info\n",
    "        experiment_key: str = None,\n",
    "        global_epoch_step: int = 0,\n",
    "        global_batch_step: int = 0,\n",
    "        global_sample_step: int = 0,\n",
    "        # stage info\n",
    "        stage_key: str = None,\n",
    "        stage_epoch_len: int = 0,\n",
    "        stage_epoch_step: int = 0,\n",
    "        stage_batch_step: int = 0,\n",
    "        stage_sample_step: int = 0,\n",
    "        # loader info\n",
    "        loader_key: str = None,\n",
    "        loader_batch_len: int = 0,\n",
    "        loader_sample_len: int = 0,\n",
    "        loader_batch_step: int = 0,\n",
    "        loader_sample_step: int = 0,\n",
    "    ) -> None:\n",
    "        if scope == \"epoch\":\n",
    "            plt.imsave(\n",
    "                os.path.join(self.logdir, f\"{tag}_{stage_key}_{stage_epoch_step}.png\"),\n",
    "                image,\n",
    "            )\n",
    "\n",
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"visualization\": VisualizationLogger(logdir=\"./logdir06/visualization\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir06/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return loaders.keys()\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders[stage]\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        return {\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir06/auc\",\n",
    "                loader_key=\"valid\", metric_key=\"auc\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir06/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"visualization\": VisualizationCallback(),\n",
    "    #         \"verbose\": TqdmCallback(),\n",
    "\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./logdir06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 7 - ``CustomRunner``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir07/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def seed(self) -> int:\n",
    "        return 73\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"experiment73\"\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"stage_1\", \"stage_2\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        if stage == \"stage_1\":\n",
    "            return {\n",
    "                \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "                \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "            }\n",
    "        elif stage == \"stage_2\":\n",
    "            return {\n",
    "                \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "                \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "            }\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        if self.model is not None:\n",
    "            return self.model\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        if stage == \"stage_1\":\n",
    "            return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "        elif stage == \"stage_2\":\n",
    "            return torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        if stage == \"stage_1\":\n",
    "            return torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 8])\n",
    "        elif stage == \"stage_2\":\n",
    "            return torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        if stage == \"stage_1\":\n",
    "            return {\n",
    "                \"criterion\": dl.CriterionCallback(\n",
    "                    metric_key=\"loss\", \n",
    "                    input_key=\"logits\", \n",
    "                    target_key=\"targets\"\n",
    "                ), \n",
    "                \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "                \"scheduler\": dl.SchedulerCallback(\n",
    "                    loader_key=\"valid\", metric_key=\"loss\"\n",
    "                ),\n",
    "                \"checkpoint\": dl.CheckpointCallback(\n",
    "                    logdir=\"./logdir07/loss\",\n",
    "                    loader_key=\"valid\", metric_key=\"loss\", \n",
    "                    minimize=True, save_n_best=3\n",
    "                ),\n",
    "            }\n",
    "        elif stage == \"stage_2\":\n",
    "            return {\n",
    "                \"auc\": dl.LoaderMetricCallback(\n",
    "                    metric=metrics.AUCMetric(),\n",
    "                    input_key=\"scores\", target_key=\"targets\", \n",
    "                ), \n",
    "                \"criterion\": dl.CriterionCallback(\n",
    "                    metric_key=\"loss\", \n",
    "                    input_key=\"logits\", \n",
    "                    target_key=\"targets\"\n",
    "                ), \n",
    "                \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "                \"scheduler\": dl.SchedulerCallback(\n",
    "                    loader_key=\"valid\", metric_key=\"loss\"\n",
    "                ),\n",
    "                \"checkpoint_auc\": dl.CheckpointCallback(\n",
    "                    logdir=\"./logdir07/auc\",\n",
    "                    loader_key=\"valid\", metric_key=\"auc\", \n",
    "                    minimize=False, save_n_best=3\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 8 - integration with hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import optuna    \n",
    "\n",
    "def objective(trial):\n",
    "    num_epochs = 6\n",
    "    num_hidden1 = int(trial.suggest_loguniform(\"num_hidden1\", 2, 16))\n",
    "    num_hidden2 = int(trial.suggest_loguniform(\"num_hidden2\", 2, 16))\n",
    "    logdir = f\"./logdir08/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    loaders = {\n",
    "        \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "        \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "        \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "    }\n",
    "\n",
    "    class CustomRunner(dl.IStageBasedRunner):\n",
    "        def get_trial(self):\n",
    "            return trial\n",
    "\n",
    "        def get_engine(self) -> dl.IEngine:\n",
    "            return dl.DeviceEngine(\"cpu\")\n",
    "\n",
    "        def get_loggers(self):\n",
    "            return {\n",
    "                \"console\": dl.ConsoleLogger(),\n",
    "                \"tensorboard\": dl.TensorboardLogger(logdir=f\"{logdir}/tb\"),\n",
    "            }\n",
    "\n",
    "        @property\n",
    "        def seed(self) -> int:\n",
    "            return 73\n",
    "\n",
    "        @property\n",
    "        def name(self) -> str:\n",
    "            return \"experiment73\"\n",
    "\n",
    "        @property\n",
    "        def stages(self) -> Iterable[str]:\n",
    "            return [\"stage_1\", \"stage_2\"]\n",
    "\n",
    "        def get_stage_len(self, stage: str) -> int:\n",
    "            return num_epochs\n",
    "\n",
    "        def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "            return loaders\n",
    "\n",
    "        def get_model(self, stage: str, epoch: int = None):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(2, num_hidden1), nn.ReLU(), \n",
    "                nn.Linear(num_hidden1, num_hidden2), nn.ReLU(), \n",
    "                nn.Linear(num_hidden2, 1)\n",
    "            )\n",
    "\n",
    "        def get_criterion(self, stage: str, epoch: int = None):\n",
    "            return nn.BCEWithLogitsLoss()\n",
    "\n",
    "        def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "            return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "        def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "            return torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "\n",
    "        def get_callbacks(self, stage: str, epoch: int = None):\n",
    "            return {\n",
    "                \"auc\": dl.LoaderMetricCallback(\n",
    "                    metric=metrics.AUCMetric(),\n",
    "                    input_key=\"scores\", target_key=\"targets\", \n",
    "                ), \n",
    "                \"criterion\": dl.CriterionCallback(\n",
    "                    metric_key=\"loss\", \n",
    "                    input_key=\"logits\", \n",
    "                    target_key=\"targets\"\n",
    "                ), \n",
    "                \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "                \"scheduler\": dl.SchedulerCallback(\n",
    "                    loader_key=\"valid\", metric_key=\"loss\"\n",
    "                ),\n",
    "                \"checkpoint\": dl.CheckpointCallback(\n",
    "                    logdir=f\"{logdir}/auc\",\n",
    "                    loader_key=\"valid\", metric_key=\"auc\", \n",
    "                    minimize=False, save_n_best=3\n",
    "                ),\n",
    "                \"optuna\": dl.OptunaPruningCallback(loader_key=\"valid\", metric_key=\"auc\", minimize=False)\n",
    "            }\n",
    "\n",
    "        def handle_batch(self, batch):\n",
    "            x, y = batch\n",
    "            y_hat = self.model(x)\n",
    "\n",
    "            self.batch = {\n",
    "                \"features\": x,\n",
    "                \"targets\": y,\n",
    "                \"logits\": y_hat.view(-1),\n",
    "                \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "            }\n",
    "\n",
    "    runner = CustomRunner()\n",
    "    runner.run()\n",
    "    score = runner.callbacks[\"optuna\"].best_score\n",
    "    \n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "#     direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=0, n_warmup_steps=0, interval_steps=1\n",
    "    ),\n",
    ")\n",
    "study.optimize(objective, n_trials=5, timeout=300)\n",
    "print(study.best_value, study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 9 - Confusion Matrix logging - IMetric+ICallback+ILogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl, metrics, utils\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 6\n",
    "num_epochs = 6\n",
    "\n",
    "class CustomSupervisedRunner(dl.IStageBasedRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"csv\": dl.CSVLogger(logdir=\"./logdir09\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir09/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return num_epochs\n",
    "    \n",
    "    def get_loaders(self, stage: str, epoch: int = None) -> \"OrderedDict[str, DataLoader]\":\n",
    "        # sample data\n",
    "        num_samples, num_features, num_classes = int(1e4), int(1e1), 6\n",
    "        X = torch.rand(num_samples, num_features)\n",
    "        y = (torch.rand(num_samples, ) * num_classes).to(torch.int64)\n",
    "\n",
    "        # pytorch loaders\n",
    "        dataset = TensorDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "        loaders = {\"train\": loader, \"valid\": loader}\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str, epoch: int = None):\n",
    "        return torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def get_criterion(self, stage: str, epoch: int = None):\n",
    "        return torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_optimizer(self, model, stage: str, epoch: int = None):\n",
    "        return torch.optim.Adam(model.parameters())\n",
    "\n",
    "    def get_scheduler(self, optimizer, stage: str, epoch: int = None):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "    \n",
    "    def get_callbacks(self, stage: str, epoch: int = None):\n",
    "        return {\n",
    "            \"accuracy\": dl.BatchMetricCallback(\n",
    "                metric=metrics.AccuracyMetric(num_classes=num_classes),\n",
    "                input_key=\"probs\", target_key=\"targets\", \n",
    "            ),\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\",\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir09/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir09/auc\",\n",
    "                loader_key=\"valid\", metric_key=\"auc\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"checkpoint3\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir9/accuracy\",\n",
    "                loader_key=\"valid\", metric_key=\"accuracy\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"verbose\": dl.TqdmCallback(),\n",
    "            \"confusion_matrix\": dl.ConfusionMatrixCallback(\n",
    "                input_key=\"probs\", \n",
    "                target_key=\"targets\",\n",
    "                prefix=\"confusion_matrix\",\n",
    "                num_classes=num_classes,\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat,\n",
    "            \"scores\": torch.sigmoid(y_hat),\n",
    "            \"probs\": torch.softmax(y_hat, dim=1),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 10 - @TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: PythonAPI is all u need\n",
    "- 10 minimal examples with different Catalyst user-friendly PythonAPI usecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start minimal examples section\n",
    "from catalyst import dl, metrics, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 11 - ML - linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl\n",
    "\n",
    "\n",
    "# data\n",
    "num_samples, num_features = int(1e4), int(1e1)\n",
    "X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir11\",\n",
    "    num_epochs=8,\n",
    "    verbose=True,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 12 - ML - multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl, metrics, utils\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 4\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (torch.rand(num_samples, ) * num_classes).to(torch.int64)\n",
    "\n",
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir12\",\n",
    "    num_epochs=6,\n",
    "    verbose=True,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    "    callbacks=[dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=num_classes)]\n",
    "#     callbacks={\n",
    "#         \"classification\": dl.BatchMetricCallback(\n",
    "#             metric=metrics.MulticlassPrecisionRecallF1SupportMetric(num_classes=num_classes),\n",
    "#             input_key=\"logits\", target_key=\"targets\", \n",
    "#         ),\n",
    "#     },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 13 - ML - multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 4\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (torch.rand(num_samples, num_classes) > 0.5).to(torch.float32)\n",
    "\n",
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_classes)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir\",\n",
    "    num_epochs=3,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    "    callbacks={\n",
    "        \"classification\": dl.BatchMetricCallback(\n",
    "            metric=metrics.MultilabelPrecisionRecallF1SupportMetric(num_classes=num_classes),\n",
    "            input_key=\"logits\", target_key=\"targets\", \n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 14 - CV - MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 15 - CV - classification with AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 16 - CV - classification with Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 17 - CV - segmentation with classification auxiliary task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst import dl, metrics\n",
    "from catalyst.contrib.data.cv import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "\n",
    "class ClassifyUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, in_hw, out_features):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, 1, 1), nn.Tanh())\n",
    "        self.decoder = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
    "        self.clf = nn.Linear(in_channels * in_hw * in_hw, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_ = z.view(z.size(0), -1)\n",
    "        y_hat = self.clf(z_)\n",
    "        x_ = self.decoder(z)\n",
    "        return y_hat, x_\n",
    "\n",
    "model = ClassifyUnet(1, 28, 10)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "    \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n",
    "}\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)\n",
    "        y_hat, x_ = self.model(x_noise)\n",
    "        \n",
    "        self.batch = {\n",
    "#             \"image\": x,\n",
    "            \"clf_targets\": y,\n",
    "            \"seg_targets\": x,\n",
    "            \"clf_logits\": y_hat,\n",
    "            \"seg_logits\": x_,\n",
    "        }\n",
    "\n",
    "\n",
    "runner = CustomRunner()\n",
    "runner.train(\n",
    "    loaders=loaders, \n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    logdir=\"./logdir14\",\n",
    "    num_epochs=6,\n",
    "    verbose=True,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    "    callbacks={\n",
    "        \"classification\": dl.BatchMetricCallback(\n",
    "            metric=metrics.MulticlassPrecisionRecallF1SupportMetric(num_classes=10),\n",
    "            input_key=\"clf_logits\", target_key=\"clf_targets\", \n",
    "        ),\n",
    "        \"segmentation\": dl.BatchMetricCallback(\n",
    "            metric=metrics.IOUMetric(),\n",
    "            input_key=\"seg_logits\", target_key=\"seg_targets\", \n",
    "        ),\n",
    "        \"criterion\": dl.CriterionCallback(\n",
    "            metric_key=\"loss\", \n",
    "            input_key=\"clf_logits\", \n",
    "            target_key=\"clf_targets\",\n",
    "        ), \n",
    "        \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 18 - CV - MNIST with Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from catalyst import data, dl, utils\n",
    "# from catalyst.contrib import datasets, models, nn\n",
    "# import catalyst.contrib.data.cv.transforms.torch as t\n",
    "\n",
    "\n",
    "# # 1. train and valid datasets\n",
    "# dataset_root = \".\"\n",
    "# transforms = t.Compose([t.ToTensor(), t.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# dataset_train = datasets.MnistMLDataset(root=dataset_root, download=True, transform=transforms)\n",
    "# sampler = data.BalanceBatchSampler(labels=dataset_train.get_labels(), p=5, k=10)\n",
    "# train_loader = DataLoader(dataset=dataset_train, sampler=sampler, batch_size=sampler.batch_size)\n",
    "\n",
    "# dataset_val = datasets.MnistQGDataset(root=dataset_root, transform=transforms, gallery_fraq=0.2)\n",
    "# val_loader = DataLoader(dataset=dataset_val, batch_size=1024)\n",
    "\n",
    "# # 2. model and optimizer\n",
    "# model = models.SimpleConv(features_dim=16)\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # 3. criterion with triplets sampling\n",
    "# sampler_inbatch = data.HardTripletsSampler(norm_required=False)\n",
    "# criterion = nn.TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)\n",
    "\n",
    "# # 4. training with catalyst Runner\n",
    "# callbacks = [\n",
    "#     dl.ControlFlowCallback(\n",
    "#         dl.CriterionCallback(input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"), \n",
    "#         loaders=\"train\"\n",
    "#     ),\n",
    "#     dl.ControlFlowCallback(dl.CMCScoreCallback(topk_args=[1]), loaders=\"valid\"),\n",
    "#     dl.PeriodicLoaderCallback(valid=100),\n",
    "# ]\n",
    "\n",
    "# runner = dl.SupervisedRunner(\n",
    "#     input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    "# )\n",
    "# runner.train(\n",
    "#     model=model,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     callbacks=callbacks,\n",
    "#     loaders={\"train\": train_loader, \"valid\": val_loader},\n",
    "#     minimize_metric=False,\n",
    "#     verbose=True,\n",
    "#     valid_loader=\"valid\",\n",
    "#     num_epochs=200,\n",
    "#     main_metric=\"cmc01\",\n",
    "# )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 19 - GAN - MNIST, flatten version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst import dl\n",
    "from catalyst.contrib.data.cv import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "from catalyst.contrib.nn.modules import Flatten, GlobalMaxPool2d, Lambda\n",
    "\n",
    "latent_dim = 128\n",
    "generator = nn.Sequential(\n",
    "    # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "    nn.Linear(128, 128 * 7 * 7),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    Lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n",
    "    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    nn.Conv2d(128, 1, (7, 7), padding=3),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    GlobalMaxPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "model = {\"generator\": generator, \"discriminator\": discriminator}\n",
    "optimizer = {\n",
    "    \"generator\": torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n",
    "    \"discriminator\": torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n",
    "}\n",
    "loaders = {\n",
    "    \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "}\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def handle_batch(self, batch):\n",
    "        real_images, _ = batch\n",
    "        batch_metrics = {}\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        batch_size = real_images.shape[0]\n",
    "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
    "        \n",
    "        # Decode them to fake images\n",
    "        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n",
    "        # Combine them with real images\n",
    "        combined_images = torch.cat([generated_images, real_images])\n",
    "        \n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = torch.cat([\n",
    "            torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))\n",
    "        ]).to(self.device)\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * torch.rand(labels.shape).to(self.device)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        predictions = self.model[\"discriminator\"](combined_images)\n",
    "        batch_metrics[\"loss_discriminator\"] = \\\n",
    "          F.binary_cross_entropy_with_logits(predictions, labels)\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = torch.zeros((batch_size, 1)).to(self.device)\n",
    "        \n",
    "        # Train the generator\n",
    "        generated_images = self.model[\"generator\"](random_latent_vectors)\n",
    "        predictions = self.model[\"discriminator\"](generated_images)\n",
    "        batch_metrics[\"loss_generator\"] = \\\n",
    "          F.binary_cross_entropy_with_logits(predictions, misleading_labels)\n",
    "        \n",
    "        self.batch_metrics.update(**batch_metrics)\n",
    "\n",
    "runner = CustomRunner()\n",
    "runner.train(\n",
    "    model=model, \n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        dl.OptimizerCallback(\n",
    "            model_key=\"generator\",\n",
    "            optimizer_key=\"generator\", \n",
    "            metric_key=\"loss_generator\"\n",
    "        ),\n",
    "        dl.OptimizerCallback(\n",
    "            model_key=\"discriminator\", \n",
    "            optimizer_key=\"discriminator\", \n",
    "            metric_key=\"loss_discriminator\"\n",
    "        ),\n",
    "    ],\n",
    "#     valid_loader=\"train\",\n",
    "#     valid_metric=\"loss_generator\",\n",
    "#     minimize_valid_metric=True,\n",
    "    num_epochs=1,\n",
    "    verbose=True,\n",
    "#     logdir=\"./logdir19\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 20 - AutoML - hyperparameters optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst import dl\n",
    "from catalyst.contrib.data.cv import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "from catalyst.contrib.nn import Flatten\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-1)\n",
    "    num_hidden = int(trial.suggest_loguniform(\"num_hidden\", 32, 128))\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "        \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n",
    "    }\n",
    "    model = nn.Sequential(\n",
    "        Flatten(), nn.Linear(784, num_hidden), nn.ReLU(), nn.Linear(num_hidden, 10)\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    runner = dl.SupervisedRunner(\n",
    "        input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    "    )\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        loaders=loaders,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        callbacks={\n",
    "            \"optuna\": dl.OptunaPruningCallback(loader_key=\"valid\", metric_key=\"accuracy01\", minimize=False, trial=trial),\n",
    "            \"accuracy\": dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n",
    "        },\n",
    "        num_epochs=10,\n",
    "#         valid_loader=\"valid\",\n",
    "#         valid_metric=\"accuracy01\",\n",
    "#         minimize_valid_metric=False,\n",
    "    )\n",
    "    score = runner.callbacks[\"optuna\"].best_score\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=1, n_warmup_steps=0, interval_steps=1\n",
    "    ),\n",
    ")\n",
    "study.optimize(objective, n_trials=10, timeout=300)\n",
    "print(study.best_value, study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 You have passed ``Kittylyst`` tutorial! This is just a minimal educational demo, but I hope you found it interesting for your deep learning research code organisation.\n",
    "\n",
    "For more advanced and production-ready solution please follow our [Catalyst](https://github.com/catalyst-team/catalyst) repository.\n",
    "\n",
    "PS. If you are interested in deep learning you could also try out our [dl-course](https://github.com/catalyst-team/dl-course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
