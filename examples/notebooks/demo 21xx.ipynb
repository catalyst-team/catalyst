{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/catalyst-team/catalyst@dev scikit-learn>=0.20 optuna --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install catalyst==21.02rc1 scikit-learn>=0.20 optuna gym==0.17.3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Catalyst 21.xx demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Customization is all u need\n",
    "- 10 minimal examples with different Catalyst customization usecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from catalyst import dl, metrics, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "def make_dataset(seed=42, n_samples=int(1e3)):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    X, y = make_moons(n_samples=n_samples, noise=0.1)\n",
    "\n",
    "    y = y*2 - 1 # make y be -1 or 1\n",
    "    return X, y\n",
    "\n",
    "def visualize_dataset(X, y):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
    "\n",
    "# let's create train data\n",
    "X_train, y_train = make_dataset()\n",
    "visualize_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid data\n",
    "X_valid, y_valid = make_dataset(seed=137)\n",
    "visualize_dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and another train one (why not?)\n",
    "X_train2, y_train2 = make_dataset(seed=1337)\n",
    "visualize_dataset(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model \n",
    "# 2-layer neural network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 16), nn.ReLU(), \n",
    "    nn.Linear(16, 16), nn.ReLU(), \n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "print(model)\n",
    "# print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_boundary(X, y, model):\n",
    "    h = 0.25\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    inputs = torch.tensor([list(xrow) for xrow in Xmesh]).float()\n",
    "    scores = model(inputs)\n",
    "    \n",
    "    Z = np.array([s.data > 0 for s in scores])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t1 = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train > 0).float())\n",
    "t2 = TensorDataset(torch.tensor(X_train2).float(), torch.tensor(y_train2 > 0).float())\n",
    "v1 = TensorDataset(torch.tensor(X_valid).float(), torch.tensor(y_valid > 0).float())\n",
    "\n",
    "loaders = {\n",
    "    \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "    \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "    \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Act 1 - ``CustomRunner – batch handling by you own``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return None\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return None\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.view(-1), y)\n",
    "        self.batch_metrics = {\"loss\": loss}\n",
    "        if self.loader_batch_step % 10 == 0:\n",
    "            print(\n",
    "                f\"{self.loader_key} ({self.loader_batch_step}/{self.loader_batch_len}:\" \n",
    "                f\"loss {loss.item()}\"\n",
    "            )\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "runner = CustomRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Act 2 - ``SupervisedRunner – Runner with Callbacks``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "    #         \"csv\": dl.LogdirLogger(logdir=\"./logdir02\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir02/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            # Let's use AUC metric as an example – it's loader-based, so we shouldn't compute it on each batch\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            # To wrap the criterion step logic, you could use CriterionCallback:\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            # To wrap the optimizer step logic, you could use OptimizerCallback:\n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            # The same case with the scheduler:\n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            # We could also use lrfinder for lr scheduling:\n",
    "#             \"lr-finder\": dl.LRFinder(\n",
    "#                 final_lr=1.0,\n",
    "#                 scale=\"log\",\n",
    "#                 num_steps=None,\n",
    "#                 optimizer_key=None,\n",
    "#             ),\n",
    "            # You can select any number of metrics to checkpoint on:\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir02/auc\",\n",
    "                loader_key=\"valid\", metric_key=\"auc\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir02/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            # Or turn on/off tqdm verbose during loader run:\n",
    "            \"verbose\": dl.TqdmCallback(),\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 3 - ``CustomMetric``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAccuracyMetric(metrics.ICallbackBatchMetric, metrics.AdditiveValueMetric):\n",
    "    def update(self, scores: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        value = ((scores > 0.5) == targets).float().mean().item()\n",
    "        value = super().update(value, len(targets))\n",
    "        return value\n",
    "    \n",
    "    def update_key_value(self, scores: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:\n",
    "        value = self.update(scores, targets)\n",
    "        return {\"accuracy\": value}\n",
    "\n",
    "    def compute_key_value(self) -> Dict[str, float]:\n",
    "        mean, std = super().compute()\n",
    "        return {\"accuracy\": mean, \"accuracy/std\": std}\n",
    "\n",
    "    \n",
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir03/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            \"accuracy\": dl.BatchMetricCallback(\n",
    "                metric=CustomAccuracyMetric(), log_on_batch=True,\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ),\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir03/accuracy\",\n",
    "                loader_key=\"valid\", metric_key=\"accuracy\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir03/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "    #         \"verbose\": dl.TqdmCallback(),\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 4 - ``CustomCallback``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the decision doundary after each epoch:\n",
    "class VisualizationCallback(dl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(order=dl.CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        img = visualize_decision_boundary(X_valid, y_valid, runner.model)\n",
    "\n",
    "\n",
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir04/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir04/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            # And include it into callbacks:        \n",
    "            \"visualization\": VisualizationCallback()\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 5 - ``CustomLogger``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_img_from_fig(fig, dpi=180):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add only a few lines to log the image to all runner's loggers\n",
    "class VisualizationCallback(dl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(order=dl.CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        image = visualize_decision_boundary(X_valid, y_valid, runner.model)\n",
    "        image = get_img_from_fig(image)\n",
    "        # runner will propagate it to all loggers\n",
    "        runner.log_image(tag=\"decision_boundary\", image=image, scope=\"epoch\")\n",
    "\n",
    "\n",
    "# Let's also add our own Logger to store image on the disk\n",
    "class VisualizationLogger(dl.ILogger):\n",
    "    def __init__(self, logdir: str):\n",
    "        self.logdir = logdir\n",
    "        os.makedirs(self.logdir, exist_ok=True)\n",
    "        \n",
    "    def log_image(\n",
    "        self,\n",
    "        tag: str,\n",
    "        image: np.ndarray,\n",
    "        scope: str = None,\n",
    "        # experiment info\n",
    "        experiment_key: str = None,\n",
    "        global_epoch_step: int = 0,\n",
    "        global_batch_step: int = 0,\n",
    "        global_sample_step: int = 0,\n",
    "        # stage info\n",
    "        stage_key: str = None,\n",
    "        stage_epoch_len: int = 0,\n",
    "        stage_epoch_step: int = 0,\n",
    "        stage_batch_step: int = 0,\n",
    "        stage_sample_step: int = 0,\n",
    "        # loader info\n",
    "        loader_key: str = None,\n",
    "        loader_batch_len: int = 0,\n",
    "        loader_sample_len: int = 0,\n",
    "        loader_batch_step: int = 0,\n",
    "        loader_sample_step: int = 0,\n",
    "    ) -> None:\n",
    "        if scope == \"epoch\":\n",
    "            plt.imsave(\n",
    "                os.path.join(self.logdir, f\"{tag}_{stage_key}_{stage_epoch_step}.png\"),\n",
    "                image,\n",
    "            )\n",
    "\n",
    "\n",
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"visualization\": VisualizationLogger(logdir=\"./logdir05/visualization\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir05/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir05/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"visualization\": VisualizationCallback()\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./logdir05\n",
    "! ls ./logdir05/loss\n",
    "! ls ./logdir05/tb\n",
    "! ls ./logdir05/visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 6 - ``Multistage Run``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_img_from_fig(fig, dpi=180):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    \n",
    "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    \"stage_1\": {\n",
    "        \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "        \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "    },\n",
    "    \"stage_2\": {\n",
    "        \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "        \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "    },\n",
    "}\n",
    "\n",
    "    \n",
    "class VisualizationCallback(dl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(order=dl.CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner):\n",
    "        image = visualize_decision_boundary(X_valid, y_valid, runner.model)\n",
    "        image = get_img_from_fig(image)\n",
    "        # runner will propagate it to all loggers\n",
    "        runner.log_image(tag=\"decision_boundary\", image=image, scope=\"epoch\")\n",
    "\n",
    "\n",
    "class VisualizationLogger(dl.ILogger):\n",
    "    def __init__(self, logdir: str):\n",
    "        self.logdir = logdir\n",
    "        os.makedirs(self.logdir, exist_ok=True)\n",
    "        \n",
    "    def log_image(\n",
    "        self,\n",
    "        tag: str,\n",
    "        image: np.ndarray,\n",
    "        scope: str = None,\n",
    "        # experiment info\n",
    "        experiment_key: str = None,\n",
    "        global_epoch_step: int = 0,\n",
    "        global_batch_step: int = 0,\n",
    "        global_sample_step: int = 0,\n",
    "        # stage info\n",
    "        stage_key: str = None,\n",
    "        stage_epoch_len: int = 0,\n",
    "        stage_epoch_step: int = 0,\n",
    "        stage_batch_step: int = 0,\n",
    "        stage_sample_step: int = 0,\n",
    "        # loader info\n",
    "        loader_key: str = None,\n",
    "        loader_batch_len: int = 0,\n",
    "        loader_sample_len: int = 0,\n",
    "        loader_batch_step: int = 0,\n",
    "        loader_sample_step: int = 0,\n",
    "    ) -> None:\n",
    "        if scope == \"epoch\":\n",
    "            plt.imsave(\n",
    "                os.path.join(self.logdir, f\"{tag}_{stage_key}_{stage_epoch_step}.png\"),\n",
    "                image,\n",
    "            )\n",
    "\n",
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"visualization\": VisualizationLogger(logdir=\"./logdir06/visualization\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir06/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return loaders.keys()\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        return loaders[stage]\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4])\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\"\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir06/auc\",\n",
    "                loader_key=\"valid\", metric_key=\"auc\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir06/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"visualization\": VisualizationCallback(),\n",
    "    #         \"verbose\": TqdmCallback(),\n",
    "\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ./logdir06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = visualize_decision_boundary(X_valid, y_valid, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 7 - ``CustomRunner``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir07/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def seed(self) -> int:\n",
    "        return 73\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"experiment73\"\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"stage_1\", \"stage_2\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return 5\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        if stage == \"stage_1\":\n",
    "            return {\n",
    "                \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "                \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "            }\n",
    "        elif stage == \"stage_2\":\n",
    "            return {\n",
    "                \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "                \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "            }\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        if self.model is not None:\n",
    "            return self.model\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(2, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 16), nn.ReLU(), \n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        if stage == \"stage_1\":\n",
    "            return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "        elif stage == \"stage_2\":\n",
    "            return torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        if stage == \"stage_1\":\n",
    "            return torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 8])\n",
    "        elif stage == \"stage_2\":\n",
    "            return torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        if stage == \"stage_1\":\n",
    "            return {\n",
    "                \"criterion\": dl.CriterionCallback(\n",
    "                    metric_key=\"loss\", \n",
    "                    input_key=\"logits\", \n",
    "                    target_key=\"targets\"\n",
    "                ), \n",
    "                \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "                \"scheduler\": dl.SchedulerCallback(\n",
    "                    loader_key=\"valid\", metric_key=\"loss\"\n",
    "                ),\n",
    "                \"checkpoint\": dl.CheckpointCallback(\n",
    "                    logdir=\"./logdir07/loss\",\n",
    "                    loader_key=\"valid\", metric_key=\"loss\", \n",
    "                    minimize=True, save_n_best=3\n",
    "                ),\n",
    "            }\n",
    "        elif stage == \"stage_2\":\n",
    "            return {\n",
    "                \"auc\": dl.LoaderMetricCallback(\n",
    "                    metric=metrics.AUCMetric(),\n",
    "                    input_key=\"scores\", target_key=\"targets\", \n",
    "                ), \n",
    "                \"criterion\": dl.CriterionCallback(\n",
    "                    metric_key=\"loss\", \n",
    "                    input_key=\"logits\", \n",
    "                    target_key=\"targets\"\n",
    "                ), \n",
    "                \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "                \"scheduler\": dl.SchedulerCallback(\n",
    "                    loader_key=\"valid\", metric_key=\"loss\"\n",
    "                ),\n",
    "                \"checkpoint_auc\": dl.CheckpointCallback(\n",
    "                    logdir=\"./logdir07/auc\",\n",
    "                    loader_key=\"valid\", metric_key=\"auc\", \n",
    "                    minimize=False, save_n_best=3\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            raise NotImplemented()\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat.view(-1),\n",
    "            \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 8 - integration with hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import optuna    \n",
    "\n",
    "def objective(trial):\n",
    "    num_epochs = 6\n",
    "    num_hidden1 = int(trial.suggest_loguniform(\"num_hidden1\", 2, 16))\n",
    "    num_hidden2 = int(trial.suggest_loguniform(\"num_hidden2\", 2, 16))\n",
    "    logdir = f\"./logdir08/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    loaders = {\n",
    "        \"train_1\": DataLoader(t1, batch_size=32, num_workers=1), \n",
    "        \"train_2\": DataLoader(t2, batch_size=32, num_workers=1), \n",
    "        \"valid\": DataLoader(v1, batch_size=32, num_workers=1), \n",
    "    }\n",
    "\n",
    "    class CustomRunner(dl.IRunner):\n",
    "        def get_trial(self):\n",
    "            return trial\n",
    "\n",
    "        def get_engine(self) -> dl.IEngine:\n",
    "            return dl.DeviceEngine(\"cpu\")\n",
    "\n",
    "        def get_loggers(self):\n",
    "            return {\n",
    "                \"console\": dl.ConsoleLogger(),\n",
    "                \"tensorboard\": dl.TensorboardLogger(logdir=f\"{logdir}/tb\"),\n",
    "            }\n",
    "\n",
    "        @property\n",
    "        def seed(self) -> int:\n",
    "            return 73\n",
    "\n",
    "        @property\n",
    "        def name(self) -> str:\n",
    "            return \"experiment73\"\n",
    "\n",
    "        @property\n",
    "        def stages(self) -> Iterable[str]:\n",
    "            return [\"stage_1\", \"stage_2\"]\n",
    "\n",
    "        def get_stage_len(self, stage: str) -> int:\n",
    "            return num_epochs\n",
    "\n",
    "        def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "            return loaders\n",
    "\n",
    "        def get_model(self, stage: str):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(2, num_hidden1), nn.ReLU(), \n",
    "                nn.Linear(num_hidden1, num_hidden2), nn.ReLU(), \n",
    "                nn.Linear(num_hidden2, 1)\n",
    "            )\n",
    "\n",
    "        def get_criterion(self, stage: str):\n",
    "            return nn.BCEWithLogitsLoss()\n",
    "\n",
    "        def get_optimizer(self, stage: str, model):\n",
    "            return torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "        def get_scheduler(self, stage: str, optimizer):\n",
    "            return torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "\n",
    "        def get_callbacks(self, stage: str):\n",
    "            return {\n",
    "                \"auc\": dl.LoaderMetricCallback(\n",
    "                    metric=metrics.AUCMetric(),\n",
    "                    input_key=\"scores\", target_key=\"targets\", \n",
    "                ), \n",
    "                \"criterion\": dl.CriterionCallback(\n",
    "                    metric_key=\"loss\", \n",
    "                    input_key=\"logits\", \n",
    "                    target_key=\"targets\"\n",
    "                ), \n",
    "                \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "                \"scheduler\": dl.SchedulerCallback(\n",
    "                    loader_key=\"valid\", metric_key=\"loss\"\n",
    "                ),\n",
    "                \"checkpoint\": dl.CheckpointCallback(\n",
    "                    logdir=f\"{logdir}/auc\",\n",
    "                    loader_key=\"valid\", metric_key=\"auc\", \n",
    "                    minimize=False, save_n_best=3\n",
    "                ),\n",
    "                \"optuna\": dl.OptunaPruningCallback(loader_key=\"valid\", metric_key=\"auc\", minimize=False)\n",
    "            }\n",
    "\n",
    "        def handle_batch(self, batch):\n",
    "            x, y = batch\n",
    "            y_hat = self.model(x)\n",
    "\n",
    "            self.batch = {\n",
    "                \"features\": x,\n",
    "                \"targets\": y,\n",
    "                \"logits\": y_hat.view(-1),\n",
    "                \"scores\": torch.sigmoid(y_hat.view(-1)),\n",
    "            }\n",
    "\n",
    "    runner = CustomRunner()\n",
    "    runner.run()\n",
    "    score = runner.callbacks[\"optuna\"].best_score\n",
    "    \n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "#     direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=0, n_warmup_steps=0, interval_steps=1\n",
    "    ),\n",
    ")\n",
    "study.optimize(objective, n_trials=5, timeout=300)\n",
    "print(study.best_value, study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 9 - Confusion Matrix logging - IMetric+ICallback+ILogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl, metrics, utils\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 6\n",
    "num_epochs = 6\n",
    "\n",
    "class CustomSupervisedRunner(dl.IRunner):\n",
    "    def get_engine(self) -> dl.IEngine:\n",
    "        return dl.DeviceEngine(\"cpu\")\n",
    "    \n",
    "    def get_loggers(self):\n",
    "        return {\n",
    "            \"console\": dl.ConsoleLogger(),\n",
    "            \"csv\": dl.CSVLogger(logdir=\"./logdir09\"),\n",
    "            \"tensorboard\": dl.TensorboardLogger(logdir=\"./logdir09/tb\"),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def stages(self) -> Iterable[str]:\n",
    "        return [\"train\"]\n",
    "    \n",
    "    def get_stage_len(self, stage: str) -> int:\n",
    "        return num_epochs\n",
    "    \n",
    "    def get_loaders(self, stage: str) -> \"OrderedDict[str, DataLoader]\":\n",
    "        # sample data\n",
    "        num_samples, num_features, num_classes = int(1e4), int(1e1), 6\n",
    "        X = torch.rand(num_samples, num_features)\n",
    "        y = (torch.rand(num_samples, ) * num_classes).to(torch.int64)\n",
    "\n",
    "        # pytorch loaders\n",
    "        dataset = TensorDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "        loaders = {\"train\": loader, \"valid\": loader}\n",
    "        return loaders\n",
    "    \n",
    "    def get_model(self, stage: str):\n",
    "        return torch.nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def get_criterion(self, stage: str):\n",
    "        return torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def get_optimizer(self, stage: str, model):\n",
    "        return torch.optim.Adam(model.parameters())\n",
    "\n",
    "    def get_scheduler(self, stage: str, optimizer):\n",
    "        return torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "    \n",
    "    def get_callbacks(self, stage: str):\n",
    "        return {\n",
    "            \"accuracy\": dl.BatchMetricCallback(\n",
    "                metric=metrics.AccuracyMetric(num_classes=num_classes),\n",
    "                input_key=\"probs\", target_key=\"targets\", \n",
    "            ),\n",
    "            \"auc\": dl.LoaderMetricCallback(\n",
    "                metric=metrics.AUCMetric(),\n",
    "                input_key=\"scores\", target_key=\"targets\", \n",
    "            ), \n",
    "            \"criterion\": dl.CriterionCallback(\n",
    "                metric_key=\"loss\", \n",
    "                input_key=\"logits\", \n",
    "                target_key=\"targets\",\n",
    "            ), \n",
    "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "            \"scheduler\": dl.SchedulerCallback(\n",
    "                loader_key=\"valid\", metric_key=\"loss\"\n",
    "            ),\n",
    "            \"checkpoint1\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir09/loss\",\n",
    "                loader_key=\"valid\", metric_key=\"loss\", \n",
    "                minimize=False, save_n_best=3\n",
    "            ),\n",
    "            \"checkpoint2\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir09/auc\",\n",
    "                loader_key=\"valid\", metric_key=\"auc\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"checkpoint3\": dl.CheckpointCallback(\n",
    "                logdir=\"./logdir9/accuracy\",\n",
    "                loader_key=\"valid\", metric_key=\"accuracy\", \n",
    "                minimize=True, save_n_best=1\n",
    "            ),\n",
    "            \"verbose\": dl.TqdmCallback(),\n",
    "            \"confusion_matrix\": dl.ConfusionMatrixCallback(\n",
    "                input_key=\"probs\", \n",
    "                target_key=\"targets\",\n",
    "                prefix=\"confusion_matrix\",\n",
    "                num_classes=num_classes,\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        \n",
    "        self.batch = {\n",
    "            \"features\": x,\n",
    "            \"targets\": y,\n",
    "            \"logits\": y_hat,\n",
    "            \"scores\": torch.sigmoid(y_hat),\n",
    "            \"probs\": torch.softmax(y_hat, dim=1),\n",
    "        }\n",
    "\n",
    "runner = CustomSupervisedRunner().run()\n",
    "model = runner.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 10 - @TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: PythonAPI is all u need\n",
    "- 10 minimal examples with different Catalyst user-friendly PythonAPI usecases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start minimal examples section\n",
    "from catalyst import dl, metrics, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 11 - ML - linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl\n",
    "\n",
    "\n",
    "# data\n",
    "num_samples, num_features = int(1e4), int(1e1)\n",
    "X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir11\",\n",
    "    num_epochs=8,\n",
    "    verbose=True,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 12 - ML - multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl, metrics, utils\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 4\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (torch.rand(num_samples, ) * num_classes).to(torch.int64)\n",
    "\n",
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir12\",\n",
    "    num_epochs=6,\n",
    "    verbose=True,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    "    callbacks=[dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=num_classes)]\n",
    "#     callbacks={\n",
    "#         \"classification\": dl.BatchMetricCallback(\n",
    "#             metric=metrics.MulticlassPrecisionRecallF1SupportMetric(num_classes=num_classes),\n",
    "#             input_key=\"logits\", target_key=\"targets\", \n",
    "#         ),\n",
    "#     },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 13 - ML - multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from catalyst import dl\n",
    "\n",
    "# sample data\n",
    "num_samples, num_features, num_classes = int(1e4), int(1e1), 4\n",
    "X = torch.rand(num_samples, num_features)\n",
    "y = (torch.rand(num_samples, num_classes) > 0.5).to(torch.float32)\n",
    "\n",
    "# pytorch loaders\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "loaders = {\"train\": loader, \"valid\": loader}\n",
    "\n",
    "# model, criterion, optimizer, scheduler\n",
    "model = torch.nn.Linear(num_features, num_classes)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])\n",
    "\n",
    "# model training\n",
    "runner = dl.SupervisedRunner(\n",
    "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    ")\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logdir\",\n",
    "    num_epochs=3,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    "    callbacks={\n",
    "        \"classification\": dl.BatchMetricCallback(\n",
    "            metric=metrics.MultilabelPrecisionRecallF1SupportMetric(num_classes=num_classes),\n",
    "            input_key=\"logits\", target_key=\"targets\", \n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 14 - CV - MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 15 - CV - classification with AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Act 16 - CV - classification with Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 17 - CV - segmentation with classification auxiliary task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst import dl, metrics\n",
    "from catalyst.data.transforms import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "\n",
    "class ClassifyUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, in_hw, out_features):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, 1, 1), nn.Tanh())\n",
    "        self.decoder = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
    "        self.clf = nn.Linear(in_channels * in_hw * in_hw, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z_ = z.view(z.size(0), -1)\n",
    "        y_hat = self.clf(z_)\n",
    "        x_ = self.decoder(z)\n",
    "        return y_hat, x_\n",
    "\n",
    "model = ClassifyUnet(1, 28, 10)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "loaders = {\n",
    "    \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "    \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n",
    "}\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def handle_batch(self, batch):\n",
    "        x, y = batch\n",
    "        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)\n",
    "        y_hat, x_ = self.model(x_noise)\n",
    "        \n",
    "        self.batch = {\n",
    "#             \"image\": x,\n",
    "            \"clf_targets\": y,\n",
    "            \"seg_targets\": x,\n",
    "            \"clf_logits\": y_hat,\n",
    "            \"seg_logits\": x_,\n",
    "        }\n",
    "\n",
    "\n",
    "runner = CustomRunner()\n",
    "runner.train(\n",
    "    loaders=loaders, \n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer, \n",
    "    logdir=\"./logdir14\",\n",
    "    num_epochs=6,\n",
    "    verbose=True,\n",
    "    valid_loader=\"valid\",\n",
    "    valid_metric=\"loss\",\n",
    "    minimize_valid_metric=True,\n",
    "    callbacks={\n",
    "        \"classification\": dl.BatchMetricCallback(\n",
    "            metric=metrics.MulticlassPrecisionRecallF1SupportMetric(num_classes=10),\n",
    "            input_key=\"clf_logits\", target_key=\"clf_targets\", \n",
    "        ),\n",
    "        \"segmentation\": dl.BatchMetricCallback(\n",
    "            metric=metrics.IOUMetric(),\n",
    "            input_key=\"seg_logits\", target_key=\"seg_targets\", \n",
    "        ),\n",
    "        \"criterion\": dl.CriterionCallback(\n",
    "            metric_key=\"loss\", \n",
    "            input_key=\"clf_logits\", \n",
    "            target_key=\"clf_targets\",\n",
    "        ), \n",
    "        \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"), \n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 18 - CV - MNIST with Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# from catalyst import data, dl, utils\n",
    "# from catalyst.contrib import datasets, models, nn\n",
    "# import catalyst.contrib.data.cv.transforms.torch as t\n",
    "\n",
    "\n",
    "# # 1. train and valid datasets\n",
    "# dataset_root = \".\"\n",
    "# transforms = t.Compose([t.ToTensor(), t.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# dataset_train = datasets.MnistMLDataset(root=dataset_root, download=True, transform=transforms)\n",
    "# sampler = data.BalanceBatchSampler(labels=dataset_train.get_labels(), p=5, k=10)\n",
    "# train_loader = DataLoader(dataset=dataset_train, sampler=sampler, batch_size=sampler.batch_size)\n",
    "\n",
    "# dataset_val = datasets.MnistQGDataset(root=dataset_root, transform=transforms, gallery_fraq=0.2)\n",
    "# val_loader = DataLoader(dataset=dataset_val, batch_size=1024)\n",
    "\n",
    "# # 2. model and optimizer\n",
    "# model = models.SimpleConv(features_dim=16)\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # 3. criterion with triplets sampling\n",
    "# sampler_inbatch = data.HardTripletsSampler(norm_required=False)\n",
    "# criterion = nn.TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)\n",
    "\n",
    "# # 4. training with catalyst Runner\n",
    "# callbacks = [\n",
    "#     dl.ControlFlowCallback(\n",
    "#         dl.CriterionCallback(input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"), \n",
    "#         loaders=\"train\"\n",
    "#     ),\n",
    "#     dl.ControlFlowCallback(dl.CMCScoreCallback(topk_args=[1]), loaders=\"valid\"),\n",
    "#     dl.PeriodicLoaderCallback(valid=100),\n",
    "# ]\n",
    "\n",
    "# runner = dl.SupervisedRunner(\n",
    "#     input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    "# )\n",
    "# runner.train(\n",
    "#     model=model,\n",
    "#     criterion=criterion,\n",
    "#     optimizer=optimizer,\n",
    "#     callbacks=callbacks,\n",
    "#     loaders={\"train\": train_loader, \"valid\": val_loader},\n",
    "#     minimize_metric=False,\n",
    "#     verbose=True,\n",
    "#     valid_loader=\"valid\",\n",
    "#     num_epochs=200,\n",
    "#     main_metric=\"cmc01\",\n",
    "# )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 19 - GAN - MNIST, flatten version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst import dl\n",
    "from catalyst.data.transforms import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "from catalyst.contrib.nn.modules import Flatten, GlobalMaxPool2d, Lambda\n",
    "\n",
    "latent_dim = 128\n",
    "generator = nn.Sequential(\n",
    "    # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "    nn.Linear(128, 128 * 7 * 7),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    Lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n",
    "    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    nn.Conv2d(128, 1, (7, 7), padding=3),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    GlobalMaxPool2d(),\n",
    "    Flatten(),\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "model = {\"generator\": generator, \"discriminator\": discriminator}\n",
    "optimizer = {\n",
    "    \"generator\": torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n",
    "    \"discriminator\": torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),\n",
    "}\n",
    "loaders = {\n",
    "    \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "}\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def handle_batch(self, batch):\n",
    "        real_images, _ = batch\n",
    "        batch_metrics = {}\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        batch_size = real_images.shape[0]\n",
    "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
    "        \n",
    "        # Decode them to fake images\n",
    "        generated_images = self.model[\"generator\"](random_latent_vectors).detach()\n",
    "        # Combine them with real images\n",
    "        combined_images = torch.cat([generated_images, real_images])\n",
    "        \n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = torch.cat([\n",
    "            torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))\n",
    "        ]).to(self.device)\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * torch.rand(labels.shape).to(self.device)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        predictions = self.model[\"discriminator\"](combined_images)\n",
    "        batch_metrics[\"loss_discriminator\"] = \\\n",
    "          F.binary_cross_entropy_with_logits(predictions, labels)\n",
    "        \n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.device)\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = torch.zeros((batch_size, 1)).to(self.device)\n",
    "        \n",
    "        # Train the generator\n",
    "        generated_images = self.model[\"generator\"](random_latent_vectors)\n",
    "        predictions = self.model[\"discriminator\"](generated_images)\n",
    "        batch_metrics[\"loss_generator\"] = \\\n",
    "          F.binary_cross_entropy_with_logits(predictions, misleading_labels)\n",
    "        \n",
    "        self.batch_metrics.update(**batch_metrics)\n",
    "\n",
    "runner = CustomRunner()\n",
    "runner.train(\n",
    "    model=model, \n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    callbacks=[\n",
    "        dl.OptimizerCallback(\n",
    "            model_key=\"generator\",\n",
    "            optimizer_key=\"generator\", \n",
    "            metric_key=\"loss_generator\"\n",
    "        ),\n",
    "        dl.OptimizerCallback(\n",
    "            model_key=\"discriminator\", \n",
    "            optimizer_key=\"discriminator\", \n",
    "            metric_key=\"loss_discriminator\"\n",
    "        ),\n",
    "    ],\n",
    "#     valid_loader=\"train\",\n",
    "#     valid_metric=\"loss_generator\",\n",
    "#     minimize_valid_metric=True,\n",
    "    num_epochs=1,\n",
    "    verbose=True,\n",
    "#     logdir=\"./logdir19\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 20 - AutoML - hyperparameters optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from catalyst import dl\n",
    "from catalyst.data.transforms import ToTensor\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "from catalyst.contrib.nn import Flatten\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-3, 1e-1)\n",
    "    num_hidden = int(trial.suggest_loguniform(\"num_hidden\", 32, 128))\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32),\n",
    "        \"valid\": DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32),\n",
    "    }\n",
    "    model = nn.Sequential(\n",
    "        Flatten(), nn.Linear(784, num_hidden), nn.ReLU(), nn.Linear(num_hidden, 10)\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    runner = dl.SupervisedRunner(\n",
    "        input_key=\"features\", output_key=\"logits\", target_key=\"targets\"\n",
    "    )\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        loaders=loaders,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        callbacks={\n",
    "            \"optuna\": dl.OptunaPruningCallback(loader_key=\"valid\", metric_key=\"accuracy01\", minimize=False, trial=trial),\n",
    "            \"accuracy\": dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n",
    "        },\n",
    "        num_epochs=3,\n",
    "#         valid_loader=\"valid\",\n",
    "#         valid_metric=\"accuracy01\",\n",
    "#         minimize_valid_metric=False,\n",
    "    )\n",
    "    score = runner.callbacks[\"optuna\"].best_score\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=1, n_warmup_steps=0, interval_steps=1\n",
    "    ),\n",
    ")\n",
    "study.optimize(objective, n_trials=3, timeout=300)\n",
    "print(study.best_value, study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Offpolicy Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import typing as tp\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "from catalyst import dl, metrics, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition', \n",
    "    field_names=[\n",
    "        'state', \n",
    "        'action', \n",
    "        'reward',\n",
    "        'done', \n",
    "        'next_state'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, size: int) -> tp.Sequence[np.array]:\n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer),\n",
    "            size,\n",
    "            replace=size > len(self.buffer)\n",
    "        )\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        states, actions, rewards, dones, next_states = (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states, dtype=np.float32)\n",
    "        )\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# as far as RL does not have some predefined dataset, \n",
    "# we need to specify epoch lenght by ourselfs\n",
    "class ReplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, epoch_size: int = int(1e3)):\n",
    "        self.buffer = buffer\n",
    "        self.epoch_size = epoch_size\n",
    "\n",
    "    def __iter__(self) -> tp.Iterator[tp.Sequence[np.array]]:\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            self.buffer.sample(self.epoch_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], next_states[i]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.epoch_size\n",
    "    \n",
    "    \n",
    "def soft_update(target: nn.Module, source: nn.Module, tau: float):\n",
    "    \"\"\"Updates the target data with smoothing by ``tau``\"\"\"\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - tau) + param.data * tau\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 21 - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    state: np.array,\n",
    "    epsilon: float = -1\n",
    ") ->  int:\n",
    "    if np.random.random() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        state = torch.tensor(state[None], dtype=torch.float32)\n",
    "        q_values = network(state).detach().cpu().numpy()[0]\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "    return int(action)\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    t_max: int = 1000,\n",
    "    epsilon: float = -1,\n",
    "    replay_buffer: tp.Optional[ReplayBuffer] = None,\n",
    ") -> tp.Tuple[float, int]:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        action = get_action(env, network, state=state, epsilon=epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if replay_buffer is not None:\n",
    "            transition = Transition(\n",
    "                state, action, reward, done, next_state)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, t\n",
    "\n",
    "def generate_sessions(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    t_max: int = 1000,\n",
    "    epsilon:float = -1,\n",
    "    replay_buffer: ReplayBuffer = None,\n",
    "    num_sessions: int = 100,\n",
    ") -> tp.Tuple[float, int]:\n",
    "    sessions_reward, sessions_steps = 0, 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        r, t = generate_session(\n",
    "            env=env, \n",
    "            network=network,\n",
    "            t_max=t_max,\n",
    "            epsilon=epsilon,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "        sessions_reward += r\n",
    "        sessions_steps += t\n",
    "    return sessions_reward, sessions_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameCallback(dl.Callback):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        env, \n",
    "        replay_buffer: ReplayBuffer,\n",
    "        session_period: int,\n",
    "        epsilon: float,\n",
    "        epsilon_k: int,\n",
    "        actor_key,\n",
    "    ):\n",
    "        super().__init__(order=0)\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.session_period = session_period\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_k = epsilon_k\n",
    "        self.actor_key = actor_key\n",
    "        self._initialized = False\n",
    "        \n",
    "\n",
    "    def on_epoch_start(self, runner: dl.IRunner):\n",
    "        self.epsilon *= self.epsilon_k\n",
    "        self.session_counter = 0\n",
    "        self.session_steps = 0\n",
    "        \n",
    "        if self._initialized:\n",
    "            return\n",
    "        \n",
    "        self.actor = runner.model[self.actor_key]\n",
    "        \n",
    "        self.actor.eval()\n",
    "        generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            epsilon=self.epsilon,\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            num_sessions=1000,\n",
    "        )\n",
    "        self.actor.train()\n",
    "        self._initialized = True\n",
    "    \n",
    "    def on_batch_end(self, runner: dl.IRunner):\n",
    "        if runner.global_batch_step % self.session_period == 0:\n",
    "            self.actor.eval()\n",
    "            \n",
    "            session_reward, session_steps = generate_session(\n",
    "                env=self.env, \n",
    "                network=self.actor,\n",
    "                epsilon=self.epsilon,\n",
    "                replay_buffer=self.replay_buffer\n",
    "            )\n",
    "\n",
    "            self.session_counter += 1\n",
    "            self.session_steps += session_steps\n",
    "\n",
    "            runner.batch_metrics.update({\"s_reward\": session_reward})\n",
    "            runner.batch_metrics.update({\"s_steps\": session_steps})\n",
    "            \n",
    "            self.actor.train()\n",
    "\n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        num_sessions = 100\n",
    "        \n",
    "        self.actor.eval()\n",
    "        valid_rewards, valid_steps = generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            num_sessions=num_sessions\n",
    "        )\n",
    "        self.actor.train()\n",
    "        \n",
    "        valid_rewards /= num_sessions\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"num_samples\"] = self.session_steps\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"updates_per_sample\"] = \\\n",
    "            runner.loader_sample_step / self.session_steps\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"v_reward\"] = valid_rewards\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"epsilon\"] = self.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(env, num_hidden: int = 128):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "\n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], num_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hidden, num_hidden),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    head = nn.Linear(num_hidden, env.action_space.n)\n",
    "\n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "\n",
    "    return torch.nn.Sequential(network, head)\n",
    "\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        gamma: float,\n",
    "        tau: float,\n",
    "        tau_period: int = 1,\n",
    "        origin_key: str = \"origin\",\n",
    "        target_key: str = \"target\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma: float = gamma\n",
    "        self.tau: float = tau\n",
    "        self.tau_period: int = tau_period\n",
    "        self.origin_key: str = origin_key\n",
    "        self.target_key: str = target_key\n",
    "        self.origin_network: nn.Module = None\n",
    "        self.target_network: nn.Module = None\n",
    "        self._initialized = False\n",
    "    \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        super().on_stage_start(runner)\n",
    "        if self._initialized:\n",
    "            return\n",
    "        self.origin_network = self.model[self.origin_key]\n",
    "        self.target_network = self.model[self.target_key]\n",
    "        soft_update(self.target_network, self.origin_network, 1.0)\n",
    "\n",
    "    def handle_batch(self, batch: tp.Sequence[np.array]):\n",
    "        # model train/valid step\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        network, target_network = self.origin_network, self.target_network\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        state_qvalues = network(states)\n",
    "        # select q-values for chosen actions\n",
    "        state_action_qvalues = \\\n",
    "            state_qvalues.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # compute q-values for all actions in next states\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        # at the last state we shall use simplified formula: \n",
    "        # Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        with torch.no_grad():\n",
    "            next_state_qvalues = target_network(next_states)\n",
    "            next_state_values = next_state_qvalues.max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        # compute \"target q-values\" for loss, \n",
    "        # it's what's inside square parentheses in the above formula.\n",
    "        target_state_action_qvalues = \\\n",
    "            next_state_values * self.gamma + rewards\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        loss = self.criterion(\n",
    "            state_action_qvalues,\n",
    "            target_state_action_qvalues.detach()\n",
    "        )\n",
    "        self.batch_metrics.update({\"loss\": loss})\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if self.global_batch_step % self.tau_period == 0:\n",
    "                soft_update(target_network, network, self.tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epoch_size = int(1e3) * batch_size\n",
    "buffer_size = int(1e5)\n",
    "# runner settings, ~training\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "tau_period = 1 # in batches\n",
    "# callback, ~exploration\n",
    "session_period = 100 # in batches\n",
    "epsilon = 0.98\n",
    "epsilon_k = 0.9\n",
    "# optimization\n",
    "lr = 3e-4\n",
    "\n",
    "# env_name = \"LunarLander-v2\"\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "network, target_network = get_network(env), get_network(env)\n",
    "utils.set_requires_grad(target_network, requires_grad=False)\n",
    "models = {\"origin\": network, \"target\": target_network}\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "loaders = {\n",
    "    \"train_game\": DataLoader(\n",
    "        ReplayDataset(replay_buffer, epoch_size=epoch_size), \n",
    "        batch_size=batch_size,\n",
    "    ),\n",
    "}\n",
    "\n",
    "runner = CustomRunner(gamma=gamma, tau=tau, tau_period=tau_period)\n",
    "runner.train(\n",
    "    model=models,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_dqn\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    valid_loader=\"_epoch_\",\n",
    "    valid_metric=\"v_reward\",\n",
    "    minimize_valid_metric=False,\n",
    "    load_best_on_end=True,\n",
    "    callbacks=[\n",
    "        GameCallback(\n",
    "            env=env, \n",
    "            replay_buffer=replay_buffer, \n",
    "            session_period=session_period,\n",
    "            epsilon=epsilon,\n",
    "            epsilon_k=epsilon_k,\n",
    "            actor_key=\"origin\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(\n",
    "    gym.make(env_name),\n",
    "    directory=\"videos_dqn\", \n",
    "    force=True)\n",
    "generate_sessions(\n",
    "    env=env, \n",
    "    network=runner.model[\"origin\"],\n",
    "    num_sessions=100\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_dqn/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 22 - DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def action(self, action: float) -> float:\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action: float) -> float:\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    state: np.array,\n",
    "    sigma: tp.Optional[float] = None\n",
    ") -> np.array:\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action = network(state).detach().cpu().numpy()[0]\n",
    "    if sigma is not None:\n",
    "        action = np.random.normal(action, sigma)\n",
    "    return action\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    sigma: tp.Optional[float] = None,\n",
    "    replay_buffer: tp.Optional[ReplayBuffer] = None,\n",
    ") -> tp.Tuple[float, int]:\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        action = get_action(env, network, state=state, sigma=sigma)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if replay_buffer is not None:\n",
    "            transition = Transition(\n",
    "                state, action, reward, done, next_state)\n",
    "            replay_buffer.append(transition)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward, t\n",
    "\n",
    "\n",
    "def generate_sessions(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    sigma: tp.Optional[float] = None,\n",
    "    replay_buffer: tp.Optional[ReplayBuffer] = None,\n",
    "    num_sessions: int = 100,\n",
    ") -> tp.Tuple[float, int]:\n",
    "    sessions_reward, sessions_steps = 0, 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        r, t = generate_session(\n",
    "            env=env, \n",
    "            network=network,\n",
    "            sigma=sigma,\n",
    "            replay_buffer=replay_buffer,\n",
    "        )\n",
    "        sessions_reward += r\n",
    "        sessions_steps += t\n",
    "    return sessions_reward, sessions_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameCallback(dl.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        *,\n",
    "        env, \n",
    "        replay_buffer: ReplayBuffer,\n",
    "        session_period: int,\n",
    "        sigma: float,\n",
    "        actor_key: str,\n",
    "    ):\n",
    "        super().__init__(order=0)\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.session_period = session_period\n",
    "        self.sigma = sigma\n",
    "        self.actor_key = actor_key\n",
    "        \n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        self.actor = runner.model[self.actor_key]\n",
    "        \n",
    "        self.actor.eval()\n",
    "        generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            sigma=self.sigma,\n",
    "            replay_buffer=self.replay_buffer,\n",
    "            num_sessions=1000,\n",
    "        )\n",
    "        self.actor.train()\n",
    "    \n",
    "    def on_epoch_start(self, runner: dl.IRunner):\n",
    "        self.session_counter = 0\n",
    "        self.session_steps = 0\n",
    "        \n",
    "    def on_batch_end(self, runner: dl.IRunner):\n",
    "        if runner.global_batch_step % self.session_period == 0:\n",
    "            self.actor.eval()\n",
    "\n",
    "            session_reward, session_steps  = generate_session(\n",
    "                env=self.env, \n",
    "                network=self.actor,\n",
    "                sigma=self.sigma,\n",
    "                replay_buffer=self.replay_buffer,\n",
    "            )\n",
    "\n",
    "            self.session_counter += 1\n",
    "            self.session_steps += session_steps\n",
    "\n",
    "            runner.batch_metrics.update({\"s_reward\": session_reward})\n",
    "            runner.batch_metrics.update({\"s_steps\": session_steps})\n",
    "\n",
    "            self.actor.train()\n",
    "            \n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        num_sessions = 100\n",
    "        \n",
    "        self.actor.eval()\n",
    "        valid_rewards, valid_steps = generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            num_sessions=num_sessions\n",
    "        )\n",
    "        self.actor.train()\n",
    "        \n",
    "        valid_rewards /= num_sessions\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"num_samples\"] = self.session_steps\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"updates_per_sample\"] = \\\n",
    "            runner.loader_sample_step / self.session_steps\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"v_reward\"] = valid_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRunner(dl.Runner):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        gamma: float,\n",
    "        tau: float,\n",
    "        tau_period: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.tau_period = tau_period\n",
    "\n",
    "    def on_stage_start(self, runner: dl.IRunner):\n",
    "        super().on_stage_start(runner)\n",
    "        soft_update(self.model[\"target_actor\"], self.model[\"actor\"], 1.0)\n",
    "        soft_update(self.model[\"target_critic\"], self.model[\"critic\"], 1.0)\n",
    "\n",
    "    def handle_batch(self, batch: tp.Sequence[torch.Tensor]):\n",
    "        # model train/valid step\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        actor, target_actor = self.model[\"actor\"], self.model[\"target_actor\"]\n",
    "        critic, target_critic = self.model[\"critic\"], self.model[\"target_critic\"]\n",
    "        actor_optimizer, critic_optimizer = self.optimizer[\"actor\"], self.optimizer[\"critic\"]\n",
    "\n",
    "        # get actions for the current state\n",
    "        pred_actions = actor(states)\n",
    "        # get q-values for the actions in current states\n",
    "        pred_critic_states = torch.cat([states, pred_actions], 1)\n",
    "        # use q-values to train the actor model\n",
    "        policy_loss = (-critic(pred_critic_states)).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get possible actions for the next states\n",
    "            next_state_actions = target_actor(next_states)\n",
    "            # get possible q-values for the next actions\n",
    "            next_critic_states = torch.cat([next_states, next_state_actions], 1)\n",
    "            next_state_values = target_critic(next_critic_states).detach().squeeze()\n",
    "            next_state_values[dones] = 0.0\n",
    "\n",
    "        # compute Bellman's equation value\n",
    "        target_state_values = next_state_values * self.gamma + rewards\n",
    "        # compute predicted values\n",
    "        critic_states = torch.cat([states, actions], 1)\n",
    "        state_values = critic(critic_states).squeeze()\n",
    "\n",
    "        # train the critic model\n",
    "        value_loss = self.criterion(\n",
    "            state_values,\n",
    "            target_state_values.detach()\n",
    "        )\n",
    "\n",
    "        self.batch_metrics.update({\n",
    "            \"critic_loss\": value_loss,\n",
    "            \"actor_loss\": policy_loss\n",
    "        })\n",
    "\n",
    "        if self.is_train_loader:\n",
    "            actor.zero_grad()\n",
    "            actor_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            critic.zero_grad()\n",
    "            critic_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            if self.global_batch_step % self.tau_period == 0:\n",
    "                soft_update(target_actor, actor, self.tau)\n",
    "                soft_update(target_critic, critic, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_actor(env):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    head = torch.nn.Sequential(\n",
    "        nn.Linear(300, 1),\n",
    "        nn.Tanh()\n",
    "    )\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "    \n",
    "    return torch.nn.Sequential(network, head)\n",
    "\n",
    "def get_network_critic(env):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.LeakyReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "    \n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0] + 1, 400),\n",
    "        nn.LeakyReLU(0.01),\n",
    "        nn.Linear(400, 300),\n",
    "        nn.LeakyReLU(0.01),\n",
    "    )\n",
    "    head = nn.Linear(300, 1)\n",
    "    \n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "    \n",
    "    return torch.nn.Sequential(network, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "batch_size = 64\n",
    "epoch_size = int(1e3) * batch_size\n",
    "buffer_size = int(1e5)\n",
    "# runner settings, ~training\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "tau_period = 1\n",
    "# callback, ~exploration\n",
    "session_period = 1\n",
    "sigma = 0.3\n",
    "# optimization\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 1e-3\n",
    "\n",
    "# You can change game\n",
    "# env_name = \"LunarLanderContinuous-v2\"\n",
    "env_name = \"Pendulum-v0\"\n",
    "env = NormalizedActions(gym.make(env_name))\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "actor, target_actor = get_network_actor(env), get_network_actor(env)\n",
    "critic, target_critic = get_network_critic(env), get_network_critic(env)\n",
    "utils.set_requires_grad(target_actor, requires_grad=False)\n",
    "utils.set_requires_grad(target_critic, requires_grad=False)\n",
    "\n",
    "models = {\n",
    "    \"actor\": actor,\n",
    "    \"critic\": critic,\n",
    "    \"target_actor\": target_actor,\n",
    "    \"target_critic\": target_critic,\n",
    "}\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = {\n",
    "    \"actor\": torch.optim.Adam(actor.parameters(), lr_actor),\n",
    "    \"critic\": torch.optim.Adam(critic.parameters(), lr=lr_critic),\n",
    "}\n",
    "\n",
    "loaders = {\n",
    "    \"train_game\": DataLoader(\n",
    "        ReplayDataset(replay_buffer, epoch_size=epoch_size), \n",
    "        batch_size=batch_size,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "runner = CustomRunner(\n",
    "    gamma=gamma, \n",
    "    tau=tau,\n",
    "    tau_period=tau_period,\n",
    ")\n",
    "\n",
    "runner.train(\n",
    "    model=models,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_ddpg\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    valid_loader=\"_epoch_\",\n",
    "    valid_metric=\"v_reward\",\n",
    "    minimize_valid_metric=False,\n",
    "    load_best_on_end=True,\n",
    "    callbacks=[\n",
    "        GameCallback(\n",
    "            env=env, \n",
    "            replay_buffer=replay_buffer, \n",
    "            session_period=session_period,\n",
    "            sigma=sigma,\n",
    "            actor_key=\"actor\",\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(\n",
    "    gym.make(env_name),\n",
    "    directory=\"videos_ddpg\", \n",
    "    force=True)\n",
    "generate_sessions(\n",
    "    env=env, \n",
    "    network=runner.model[\"actor\"],\n",
    "    num_sessions=100\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_ddpg/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Onpolicy Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Act 23 - REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import typing as tp\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "from catalyst import dl, metrics, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Rollout = namedtuple(\n",
    "    'Rollout', \n",
    "    field_names=[\n",
    "        'states', \n",
    "        'actions', \n",
    "        'rewards',\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def append(self, rollout: Rollout):\n",
    "        self.buffer.append(rollout)\n",
    "    \n",
    "    def sample(self, idx: int) -> tp.Sequence[np.array]:\n",
    "        states, actions, rewards = self.buffer[idx]\n",
    "        states, actions, rewards = (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.int64),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "        )\n",
    "        return states, actions, rewards\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# as far as RL does not have some predefined dataset, \n",
    "# we need to specify epoch lenght by ourselfs\n",
    "class RolloutDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer: RolloutBuffer):\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def __iter__(self) -> tp.Iterator[tp.Sequence[np.array]]:\n",
    "        for i in range(len(self.buffer)):\n",
    "            states, actions, rewards = self.buffer.sample(i)\n",
    "            yield states, actions, rewards\n",
    "        self.buffer.buffer.clear()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.buffer.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma = 0.99):    \n",
    "    G = [rewards[-1]]\n",
    "    for r in reversed(rewards[:-1]):\n",
    "        G.insert(0, r + gamma * G[0])\n",
    "    return G\n",
    "\n",
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Takes an integer vector and converts it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot\n",
    "\n",
    "def get_action(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    state: np.array,\n",
    "    epsilon: float = -1\n",
    ") ->  int:\n",
    "#     if np.random.random() < epsilon:\n",
    "#         action = env.action_space.sample()\n",
    "#     else:\n",
    "    state = torch.tensor(state[None], dtype=torch.float32)\n",
    "    logits = network(state).detach()\n",
    "    probas = F.softmax(logits, -1).cpu().numpy()[0]\n",
    "    action = np.random.choice(len(probas), p=probas)\n",
    "    return int(action)\n",
    "\n",
    "\n",
    "def generate_session(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    t_max: int = 1000,\n",
    "    epsilon: float = -1,\n",
    "    rollout_buffer: tp.Optional[RolloutBuffer] = None,\n",
    ") -> tp.Tuple[float, int]:\n",
    "    total_reward = 0\n",
    "    states, actions, rewards = [], [], []\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        action = get_action(env, network, state=state, epsilon=epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    if rollout_buffer is not None:\n",
    "        rollout_buffer.append(Rollout(states, actions, rewards))\n",
    "            \n",
    "    return total_reward, t\n",
    "\n",
    "def generate_sessions(\n",
    "    env,\n",
    "    network: nn.Module,\n",
    "    t_max: int = 1000,\n",
    "    epsilon:float = -1,\n",
    "    rollout_buffer: tp.Optional[RolloutBuffer] = None,\n",
    "    num_sessions: int = 100,\n",
    ") -> tp.Tuple[float, int]:\n",
    "    sessions_reward, sessions_steps = 0, 0\n",
    "    for i_episone in range(num_sessions):\n",
    "        r, t = generate_session(\n",
    "            env=env, \n",
    "            network=network,\n",
    "            t_max=t_max,\n",
    "            epsilon=epsilon,\n",
    "            rollout_buffer=rollout_buffer,\n",
    "        )\n",
    "        sessions_reward += r\n",
    "        sessions_steps += t\n",
    "    return sessions_reward, sessions_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GameCallback(dl.Callback):\n",
    "    \n",
    "    def __init__(self,  *,  env, rollout_buffer: RolloutBuffer):\n",
    "        super().__init__(order=0)\n",
    "        self.env = env\n",
    "        self.rollout_buffer = rollout_buffer\n",
    "\n",
    "    def on_epoch_start(self, runner: dl.IRunner):\n",
    "        self.actor = runner.model\n",
    "        \n",
    "        self.actor.eval()\n",
    "        generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            rollout_buffer=self.rollout_buffer,\n",
    "            num_sessions=100,\n",
    "        )\n",
    "        self.actor.train()\n",
    "\n",
    "    def on_epoch_end(self, runner: dl.IRunner):\n",
    "        num_sessions = 100\n",
    "        \n",
    "        self.actor.eval()\n",
    "        valid_rewards, valid_steps = generate_sessions(\n",
    "            env=self.env, \n",
    "            network=self.actor,\n",
    "            num_sessions=num_sessions\n",
    "        )\n",
    "        self.actor.train()\n",
    "        \n",
    "        valid_rewards /= num_sessions\n",
    "        runner.epoch_metrics[\"_epoch_\"][\"v_reward\"] = valid_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_network(env, num_hidden: int = 128):\n",
    "    inner_fn = utils.get_optimal_inner_init(nn.ReLU)\n",
    "    outer_fn = utils.outer_init\n",
    "\n",
    "    network = torch.nn.Sequential(\n",
    "        nn.Linear(env.observation_space.shape[0], num_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(num_hidden, num_hidden),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    head = nn.Linear(num_hidden, env.action_space.n)\n",
    "\n",
    "    network.apply(inner_fn)\n",
    "    head.apply(outer_fn)\n",
    "\n",
    "    return torch.nn.Sequential(network, head)\n",
    "\n",
    "\n",
    "class CustomRunner(dl.Runner):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        gamma: float,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma: float = gamma\n",
    "        self._initialized = False\n",
    "\n",
    "    def handle_batch(self, batch: tp.Sequence[np.array]):\n",
    "        # model train/valid step\n",
    "        # ATTENTION: \n",
    "        # because of different trajectories lens\n",
    "        # ONLY batch_size==1 supported\n",
    "        states, actions, rewards = batch\n",
    "        states, actions, rewards = states[0], actions[0], rewards[0]\n",
    "        cumulative_returns = torch.tensor(get_cumulative_rewards(rewards, gamma))\n",
    "        network = self.model\n",
    "\n",
    "        logits = network(states)\n",
    "        probas = F.softmax(logits, -1)\n",
    "        logprobas = F.log_softmax(logits, -1)\n",
    "        n_actions = probas.shape[1]\n",
    "        logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions, n_dims=n_actions), dim=1)\n",
    "        \n",
    "        J_hat = torch.mean(logprobas_for_actions * cumulative_returns)\n",
    "        entropy_reg = - torch.mean(torch.sum(probas * logprobas, dim = 1))\n",
    "        loss = - J_hat - 0.1 * entropy_reg\n",
    "\n",
    "        self.batch_metrics.update({\"loss\": loss})\n",
    "        if self.is_train_loader:\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epoch_size = int(1e3) * batch_size\n",
    "buffer_size = int(1e2)\n",
    "# runner settings\n",
    "gamma = 0.99\n",
    "# optimization\n",
    "lr = 3e-4\n",
    "\n",
    "# env_name = \"LunarLander-v2\"\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "rollout_buffer = RolloutBuffer(buffer_size)\n",
    "\n",
    "model = get_network(env)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loaders = {\n",
    "    \"train_game\": DataLoader(\n",
    "        RolloutDataset(rollout_buffer), \n",
    "        batch_size=batch_size,\n",
    "    ),\n",
    "}\n",
    "\n",
    "runner = CustomRunner(gamma=gamma)\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loaders=loaders,\n",
    "    logdir=\"./logs_dqn\",\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    valid_loader=\"_epoch_\",\n",
    "    valid_metric=\"v_reward\",\n",
    "    minimize_valid_metric=False,\n",
    "    load_best_on_end=True,\n",
    "    callbacks=[\n",
    "        GameCallback(\n",
    "            env=env, \n",
    "            rollout_buffer=rollout_buffer, \n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "\n",
    "env = gym.wrappers.Monitor(\n",
    "    gym.make(env_name),\n",
    "    directory=\"videos_reinforce\", \n",
    "    force=True)\n",
    "generate_sessions(\n",
    "    env=env, \n",
    "    network=model,\n",
    "    num_sessions=100\n",
    ")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos_reinforce/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
