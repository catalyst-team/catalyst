{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRWGyaT2Y25j"
   },
   "source": [
    "# Catalyst tiled inference tutorial\n",
    "\n",
    "Authors: [Pavel Danilov](https://github.com/pdanilov), [Sergey Kolesnilov](https://github.com/scitator)\n",
    "\n",
    "[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n",
    "\n",
    "### Colab setup\n",
    "\n",
    "First of all, do not forget to change the runtime type to GPU. <br/>\n",
    "To do so click `Runtime` -> `Change runtime type` -> Select `\"Python 3\"` and `\"GPU\"` -> click `Save`. <br/>\n",
    "After that you can click `Runtime` -> `Run` all and watch the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHJAs8U5Y25m"
   },
   "source": [
    "\n",
    "## Requirements\n",
    "\n",
    "Download and install the latest version of catalyst and other libraries required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "xCoyLtaeY25m",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for augmentations\n",
    "!pip install albumentations==0.4.3\n",
    "\n",
    "# for pretrained segmentation models for PyTorch\n",
    "!pip install segmentation-models-pytorch==0.1.0\n",
    "\n",
    "################\n",
    "# Catalyst itself\n",
    "!pip install -U catalyst\n",
    "# For specific version of catalyst, uncomment:\n",
    "# ! pip install git+http://github.com/catalyst-team/catalyst.git@{master/commit_hash}\n",
    "################\n",
    "\n",
    "# for tensorboard\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MncoA-G0Y25p"
   },
   "source": [
    "### Colab extras – Plotly\n",
    "\n",
    "To intergate visualization library `plotly` to colab, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "def configure_plotly_browser_state():\n",
    "    display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "        <script>\n",
    "          requirejs.config({\n",
    "            paths: {\n",
    "              base: '/static/base',\n",
    "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
    "            },\n",
    "          });\n",
    "        </script>\n",
    "        '''))\n",
    "\n",
    "\n",
    "IPython.get_ipython().events.register('pre_run_cell', configure_plotly_browser_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8H65wGVbY25q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import torch\n",
    "import catalyst\n",
    "\n",
    "from catalyst.dl import utils\n",
    "\n",
    "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
    "\n",
    "SEED = 42\n",
    "utils.set_global_seed(SEED)\n",
    "utils.prepare_cudnn(deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWseoJqWY25z"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "As a dataset we will take Carvana - binary segmentation for the \"car\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34H0dXBYZepr"
   },
   "source": [
    "> If you are on MacOS and you don’t have `wget`, you can install it with: `brew install wget`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Catalyst installation, `download-gdrive` function become available to download objects from Google Drive.\n",
    "We use it to download datasets.\n",
    "\n",
    "usage: `download-gdrive {FILE_ID} {FILENAME}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some issues during executing cell below, just try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8H8tDrZ6Y250"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATA_ARCHIVE=segmentation_data.zip\n",
    "if [ ! -f \"${DATA_ARCHIVE}\" ]; then\n",
    "    download-gdrive 1iYaNijLmzsrMlAdMoUEhhJuo-5bkeAuj \"${DATA_ARCHIVE}\" &> /dev/null\n",
    "    extract-archive \"${DATA_ARCHIVE}\" &> /dev/null\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4Vqm9FzY254"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"segmentation_data\")\n",
    "\n",
    "train_image_path = ROOT / \"train\"\n",
    "train_mask_path = ROOT / \"train_masks\"\n",
    "test_image_path = ROOT / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vL0sd6__M_98"
   },
   "source": [
    "Collect images and masks into variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wU9IPpyAfhOy"
   },
   "outputs": [],
   "source": [
    "ALL_IMAGES = sorted(train_image_path.glob(\"*.jpg\"))\n",
    "len(ALL_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ElvJQrlOf4Gm"
   },
   "outputs": [],
   "source": [
    "ALL_MASKS = sorted(train_mask_path.glob(\"*.gif\"))\n",
    "len(ALL_MASKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES = sorted(test_image_path.glob(\"*.jpg\"))\n",
    "len(TEST_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mcVBd_WjI43"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.io import imread as gif_imread\n",
    "from catalyst import utils\n",
    "\n",
    "\n",
    "def show_examples(name: str, image: np.ndarray, mask: np.ndarray, mask_cmap_gray: bool = True):\n",
    "    plt.figure(figsize=(10, 14))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Image: {name}\")\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cmap = \"gray\" if mask_cmap_gray else None\n",
    "    plt.imshow(mask, cmap=cmap)\n",
    "    plt.title(f\"Mask: {name}\")\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "\n",
    "def show(index: int, images: List[Path], masks: List[Path], transforms=None) -> None:\n",
    "    image_path = images[index]\n",
    "    name = image_path.name\n",
    "\n",
    "    image = utils.imread(image_path)\n",
    "    mask = gif_imread(masks[index])\n",
    "\n",
    "    if transforms is not None:\n",
    "        temp = transforms(image=image, mask=mask)\n",
    "        image = temp[\"image\"]\n",
    "        mask = temp[\"mask\"]\n",
    "\n",
    "    show_examples(name, image, mask)\n",
    "\n",
    "def show_random(images: List[Path], masks: List[Path], transforms=None) -> None:\n",
    "    length = len(images)\n",
    "    index = random.randint(0, length - 1)\n",
    "    show(index, images, masks, transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0zVSTYtk8hf"
   },
   "source": [
    "You can restart the cell below to see more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0EZVF1pk3tC"
   },
   "outputs": [],
   "source": [
    "show_random(ALL_IMAGES, ALL_MASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpEVELsNNMTO"
   },
   "source": [
    "The dataset below reads images and masks and optionally applies augmentation to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Grrv0FqpY25-"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: List[Path],\n",
    "        masks: List[Path] = None,\n",
    "        transforms=None\n",
    "    ) -> None:\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        image_path = self.images[idx]\n",
    "        image = utils.imread(image_path)\n",
    "        \n",
    "        result = {\"image\": image}\n",
    "        \n",
    "        if self.masks is not None:\n",
    "            mask = gif_imread(self.masks[idx])\n",
    "            result[\"mask\"] = mask\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            result = self.transforms(**result)\n",
    "        \n",
    "        result[\"filename\"] = image_path.name\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsI3ZS2asQqg"
   },
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RF0wtzsjNZQ5"
   },
   "source": [
    "[![Albumentation logo](https://albumentations.readthedocs.io/en/latest/_static/logo.png)](https://github.com/albu/albumentations)\n",
    "\n",
    "The [albumentation](https://github.com/albu/albumentations) library works with images and masks at the same time, which is what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNdK5P0UY26A"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "\n",
    "def hard_transforms():\n",
    "    result = [\n",
    "        albu.Cutout(),\n",
    "        albu.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2, contrast_limit=0.2, p=0.3\n",
    "        ),\n",
    "        albu.GridDistortion(p=0.3),\n",
    "        albu.HueSaturationValue(p=0.3),\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "\n",
    "def resize_transforms(pre_size=448, image_size=448, train=True):\n",
    "    result = [\n",
    "        albu.RandomScale(scale_limit=(0.8, 1.25), p=1 if train else 0),\n",
    "        albu.RandomCrop(pre_size, pre_size, p=1 if train else 0),\n",
    "        albu.CenterCrop(pre_size, pre_size, p=0 if train else 1),\n",
    "        albu.Resize(image_size, image_size, p=1),\n",
    "    ]\n",
    "    return result\n",
    "  \n",
    "    \n",
    "def post_transforms():\n",
    "    # we use ImageNet image normalization\n",
    "    # and convert it to torch.Tensor\n",
    "    return [albu.Normalize(), ToTensor()]\n",
    "\n",
    "\n",
    "def compose(*transforms_to_compose, p=1):\n",
    "    # combine all augmentations into one single pipeline\n",
    "    transforms_to_compose = chain.from_iterable(transforms_to_compose)\n",
    "    result = albu.Compose([*transforms_to_compose], p=p)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrRzeFppnPMt"
   },
   "outputs": [],
   "source": [
    "show_transforms = compose(\n",
    "    hard_transforms(),\n",
    "    resize_transforms(train=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTiQd-frN74v"
   },
   "source": [
    "Let's look at the augmented results. <br/>\n",
    "You can restart the cell below to see more examples of augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gGIbhBm1orXt"
   },
   "outputs": [],
   "source": [
    "show_random(ALL_IMAGES, ALL_MASKS, transforms=show_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvLlx1OvosGX"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BF2Vgdly9RR"
   },
   "source": [
    "## Experiment\n",
    "### Model\n",
    "\n",
    "Catalyst has [several segmentation models](https://github.com/catalyst-team/catalyst/blob/master/catalyst/contrib/models/segmentation/__init__.py#L16) (Unet, Linknet, FPN, PSPnet and their versions with pretrain from Resnet).\n",
    "\n",
    "> You can read more about them in [our blog post](https://github.com/catalyst-team/catalyst-info#catalyst-info-1-segmentation-models).\n",
    "\n",
    "But for now let's take the model from [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) (SMP for short). The same segmentation architectures have been implemented in this repository, but there are many more pre-trained encoders.\n",
    "\n",
    "[![Segmentation Models logo](https://raw.githubusercontent.com/qubvel/segmentation_models.pytorch/master/pics/logo-small-w300.png)](https://github.com/qubvel/segmentation_models.pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zm7JsNrczOQG"
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# We will use Feature Pyramid Network with pre-trained ResNeXt50 backbone\n",
    "model = smp.FPN(encoder_name=\"resnext50_32x4d\", classes=1)\n",
    "init_state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "logdir = \"./logs/segmentation\"\n",
    "\n",
    "device = utils.get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# by default SupervisedRunner uses \"features\" and \"targets\",\n",
    "# in our case we get \"image\" and \"mask\" keys in dataset __getitem__\n",
    "runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiled Inference\n",
    "This tutorial is all about inference, so we provide you a checkpoint for trained model, which you can use to watch the results of inference\n",
    "\n",
    "The main purpose of the method is allow inference on high-resolution image which doesn't fit into GPU memory\n",
    "\n",
    "The main idea of the method is to split image into square patches with intersection, do inference on each of them and then calculate weighted sum of predictions to form resulting mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some issues during executing cell below, just try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "CHECKPOINT=tiled_inference_best.pth\n",
    "if [ ! -f \"${CHECKPOINT}\" ]; then\n",
    "    download-gdrive 16qfA51v8ver0wUjyT7HSEZoA69U6uSbs \"${CHECKPOINT}\" &> /dev/null\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Full HD images, we will use relatively big patches of size 448, stepping from one to another by 224 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from skimage.io import imread\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from catalyst.contrib.data.cv.datasets import TiledImageDataset\n",
    "from catalyst.contrib.dl.callbacks.tiled_inference import TiledInferenceCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 5\n",
    "infer_transforms = compose(post_transforms())\n",
    "infer_images = TEST_IMAGES[:NUM_IMAGES]\n",
    "\n",
    "infer_dataset = TiledImageDataset(\n",
    "    images=infer_images,\n",
    "    train=False,\n",
    "    tile_size=448,\n",
    "    tile_step=224,\n",
    "    input_key=\"image\",\n",
    "    transform=infer_transforms,\n",
    ")\n",
    "\n",
    "batch_size: int = 32\n",
    "\n",
    "infer_loader = DataLoader(\n",
    "    infer_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "runner.infer(\n",
    "    model=model,\n",
    "    loaders=collections.OrderedDict(infer=infer_loader),\n",
    "    # we use relatively high threshold of 0.9 instead of default 0.5 to avoid inference artifacts\n",
    "    callbacks=[TiledInferenceCallback(save_dir=\"predictions\", threshold=0.9)],\n",
    "    # use default option to load pre-trainded checkpoint\n",
    "    resume=\"tiled_inference_best.pth\",\n",
    "    # or train model by yourself and use option below\n",
    "    # resume=f\"{logdir}/checkpoints/best.pth\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are ready to watch segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = np.load(\"predictions/masks.npy\")\n",
    "\n",
    "for i, image_path in enumerate(infer_images):\n",
    "    image = imread(image_path)\n",
    "    mask = masks[i]\n",
    "    show_examples(name=\"\", image=image, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can watch onto probability heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.load(\"predictions/probs.npy\")\n",
    "\n",
    "for i, image_path in enumerate(infer_images):\n",
    "    image = imread(image_path)\n",
    "    prob = probs[i][0]\n",
    "    show_examples(name=\"\", image=image, mask=prob, mask_cmap_gray=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The results of inference is quite good, but you may think is's not perfect, and then why should I use this method instead of downscale-inference-upscale? The answer is, you can tune your model more, and obtain a scalable solution that works for 4K or even 8K images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional\n",
    "You can train the model by youself and reproduce the results\n",
    "\n",
    "Note: this option requires huge amount of computational resources: reference solution was trained on TITAN V (32 Gb VRAM) or you can try to use multiple GPU machine, but the result may be worse, because DataParallel module averages agradients from GPUs, on each of them batch size will have size around 8-12, that seems to be too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGotRWGjyYOw"
   },
   "source": [
    "### Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIfyQYAhY26C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_loaders(\n",
    "    images: List[Path],\n",
    "    masks: List[Path],\n",
    "    random_state: int,\n",
    "    valid_size: float = 0.2,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 4,\n",
    "    train_transforms_fn = None,\n",
    "    valid_transforms_fn = None,\n",
    ") -> dict:\n",
    "\n",
    "    indices = np.arange(len(images))\n",
    "\n",
    "    # Let's divide the data set into train and valid parts.\n",
    "    train_indices, valid_indices = train_test_split(\n",
    "        indices, test_size=valid_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    np_images = np.array(images)\n",
    "    np_masks = np.array(masks)\n",
    "\n",
    "    # Creates our train dataset\n",
    "    train_dataset = SegmentationDataset(\n",
    "        images=np_images[train_indices].tolist(),\n",
    "        masks=np_masks[train_indices].tolist(),\n",
    "        transforms=train_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Creates our valid dataset\n",
    "    valid_dataset = SegmentationDataset(\n",
    "        images=np_images[valid_indices].tolist(),\n",
    "        masks=np_masks[valid_indices].tolist(),\n",
    "        transforms=valid_transforms_fn\n",
    "    )\n",
    "\n",
    "    # Catalyst uses normal torch.data.DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    \n",
    "    loaders = collections.OrderedDict(train=train_loader, valid=valid_loader)\n",
    "    \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXxJFBUkybYs"
   },
   "source": [
    "### Training preparation\n",
    "We will train our model on square patches with crop size 448.\n",
    "This configuration of parameters wat tested on TITAN V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 6\n",
    "batch_size: int = 48\n",
    "\n",
    "train_transforms = compose(\n",
    "    resize_transforms(),\n",
    "    hard_transforms(),\n",
    "    post_transforms(),\n",
    ")\n",
    "valid_transforms = compose(\n",
    "    resize_transforms(),\n",
    "    post_transforms(),\n",
    ")    \n",
    "\n",
    "loaders = get_loaders(\n",
    "    images=ALL_IMAGES,\n",
    "    masks=ALL_MASKS,\n",
    "    random_state=SEED,\n",
    "    train_transforms_fn=train_transforms,\n",
    "    valid_transforms_fn=valid_transforms,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IirWWkf8PeXh"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "from catalyst.contrib.nn import RAdam, Lookahead\n",
    "\n",
    "learning_rate = 0.001\n",
    "encoder_learning_rate = 0.0005\n",
    "\n",
    "# Since we use a pre-trained encoder, we will reduce the learning rate on it.\n",
    "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n",
    "\n",
    "# This function removes weight_decay for biases and applies our layerwise_params\n",
    "model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n",
    "\n",
    "# Catalyst has new SOTA optimizers out of box\n",
    "base_optimizer = RAdam(model_params, lr=learning_rate, weight_decay=0.0003)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "We will optimize loss as the sum of IoU, Dice and BCE, specifically this function: $IoU + Dice + 0.8*BCE$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from catalyst.dl.callbacks import DiceCallback, IouCallback, CriterionCallback, MetricAggregationCallback\n",
    "from catalyst.contrib.nn import DiceLoss, IoULoss\n",
    "\n",
    "# we have multiple criterions\n",
    "criterion = {\n",
    "    \"dice\": DiceLoss(),\n",
    "    \"iou\": IoULoss(),\n",
    "    \"bce\": nn.BCEWithLogitsLoss()\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    # Each criterion is calculated separately.\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_dice\",\n",
    "        criterion_key=\"dice\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_iou\",\n",
    "        criterion_key=\"iou\"\n",
    "    ),\n",
    "    CriterionCallback(\n",
    "        input_key=\"mask\",\n",
    "        prefix=\"loss_bce\",\n",
    "        criterion_key=\"bce\"\n",
    "    ),\n",
    "\n",
    "    # And only then we aggregate everything into one loss.\n",
    "    MetricAggregationCallback(\n",
    "        prefix=\"loss\",\n",
    "        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n",
    "        # because we want weighted sum, we need to add scale for each loss\n",
    "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
    "    ),\n",
    "\n",
    "    # metrics\n",
    "    DiceCallback(input_key=\"mask\"),\n",
    "    IouCallback(input_key=\"mask\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqBlC5_iY26K"
   },
   "source": [
    "### Running train-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(init_state_dict)\n",
    "\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    # our dataloaders\n",
    "    loaders=loaders,\n",
    "    # We can specify the callbacks list for the experiment;\n",
    "    callbacks=callbacks,\n",
    "    # path to save logs\n",
    "    logdir=logdir,\n",
    "    num_epochs=num_epochs,\n",
    "    # save our best checkpoint by IoU metric\n",
    "    main_metric=\"iou\",\n",
    "    # IoU needs to be maximized.\n",
    "    minimize_metric=False,\n",
    "    # prints train logs\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your checkpoint is located at logs/segmentation/checkpoints/best.pth and you can also use it for making inference"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "segmentation-tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
