# For detailed comments on this configuration file refer to configs/hydra-description-eng.yml

shared:
  dataset_root: './data'
  seed: 42
  valid_loader: valid
  valid_metric: accuracy01
  minimize_valid_metric: False

  optimizer:
    target: torch.optim.Adam
    lr: 0.001
    weight_decay: 0.0001

  scheduler:
    target: catalyst.contrib.nn.OneCycleLRWithWarmup
    warmup_steps: 1
    lr_range: [ 0.005, 0.00005 ]
    momentum_range: [ 0.85, 0.95 ]

args:
  expdir: "cifar_stages"
  logdir: "./logs/cifar_stages_hydra"
  seed: ${shared.seed}
  distributed: False
  apex: False
  amp: False
  verbose: False
  timeit: False
  check: False
  overfit: False
  deterministic: False
  benchmark: False

runner:
  _target_: cifar_stages.CustomSupervisedHydraRunner
  input_key: "features"
  output_key: "logits"
  target_key: "targets"
  loss_key: "loss"

model:
  _target_: cifar_stages.SimpleNet

stages:
  train:
    num_epochs: 10

    loaders:
      batch_size: ${shared.train_batch_size}
      num_workers: ${shared.train_num_workers}
#      train:
#        batch_size: ${shared.train_batch_size}
#        num_workers: ${shared.train_num_workers}
#      valid:
#        batch_size: ${shared.valid_batch_size}
#        num_workers: ${shared.valid_num_workers}

#      datasets:
#        train:
#          _target_: catalyst.contrib.datasets.MNIST
#          root: ${shared.dataset_root}
#          train: True
#          download: True
#        valid:
#          _target_: catalyst.contrib.datasets.MNIST
#          root: ${shared.dataset_root}
#          train: False
#          download: True
#
#      transforms:
#        train:
#          _target_: _tests_cv_classification_hydra.simple_transform
#        valid:
#          _target_: _tests_cv_classification_hydra.simple_transform

    criterion:
      _target_: torch.nn.CrossEntropyLoss

    optimizer:
      _target_: ${shared.optimizer.target}
      lr: ${shared.optimizer.lr}
      weight_decay: ${shared.optimizer.weight_decay}

    scheduler:
      _target_: ${shared.scheduler.target}
      num_steps: ${stages.train.num_epochs}
      warmup_steps: ${shared.scheduler.warmup_steps}
      lr_range: ${shared.scheduler.lr_range}
      momentum_range: ${shared.scheduler.momentum_range}

    callbacks:
      accuracy:
        _target_: catalyst.callbacks.AccuracyCallback
        input_key: ${runner.output_key}
        target_key: ${runner.target_key}
        accuracy_args: [ 1, 3, 5 ]
      loss:
        _target_: catalyst.callbacks.CriterionCallback
        input_key: ${runner.output_key}
        target_key: ${runner.target_key}
        metric_key: ${runner.loss_key}
      optimizer:
        _target_: catalyst.callbacks.OptimizerCallback
        metric_key: ${runner.loss_key}
      scheduler:
        _target_: catalyst.callbacks.SchedulerCallback
        loader_key: ${shared.valid_loader}
        metric_key: ${shared.valid_metric}
      saver:
        _target_: catalyst.callbacks.CheckpointCallback
        logdir: ${args.logdir}
        loader_key: ${shared.valid_loader}
        metric_key: ${shared.valid_metric}
        minimize: ${shared.minimize_valid_metric}
        save_n_best: 3
      verbose:
        _target_: catalyst.callbacks.TqdmCallback

#  tune:
#    num_epochs: 6
#
#    loaders: ${stages.train.loaders}
#
#    criterion:
#      _target_: torch.nn.CrossEntropyLoss
#
#    optimizer:
#      _target_: catalyst.contrib.nn.Ralamb
#
#    scheduler:
#      _target_: torch.optim.lr_scheduler.MultiStepLR
#      milestones: [ 3 ]
#      gamma: 0.3
#
#    callbacks: ${stages.train.callbacks}

#hydra:
#  run:
#    dir: ${args.logdir}
