args:
  logdir: ./logs/mujoco-cheetah-td3  #  change me
  expdir: null

  vis: 0
  infer: 1  #  change me
  train: 4  #  change me

db:
  port: 12000
  prefix: mujoco-td3  # TODO: remove

environment:
  environment: GymWrapper
  env_name: HalfCheetah-v2
  history_len: 3
  frame_skip: 1
  reward_scale: 1.0
  step_delay: 0.008

agents:
  actor:
    agent: Actor

    state_net_params:  # state -> hidden representation
      observation_net_params:
        hiddens: [128]  # first hidden would be taken from state_shape
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false
      main_net_params:
        hiddens: [128, 128]
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false
    policy_head_params:  # hidden representation -> ~policy
      in_features: 128  # out features would be taken from action_shape
      policy_type: null
      out_activation: Tanh

  critic:
    agent: StateActionCritic

    state_action_net_params:  # state -> hidden representation
      observation_net_params:
        hiddens: [64]  # first hidden would be taken from state_shape
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false
      action_net_params:
        hiddens: [64]  # first hidden would be taken from action_shape
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false
      main_net_params:
        hiddens: [128, 128]
        layer_fn: Linear
        norm_fn: LayerNorm
        activation_fn: ReLU
        bias: false

    value_head_params:  # hidden representation -> value
      in_features: 128  # out features would be taken from action_shape
      num_atoms: 1

#      distribution: categorical
#      num_atoms: 101
#      values_range: [-1000.0, 1000.0]

#       distribution: quantile
#       num_atoms: 128

algorithm:
  algorithm: TD3

  n_step: 1
  gamma: 0.99
  actor_tau: 0.01
  critic_tau: 0.01

  num_critics: 2
  action_noise_std: 0.1
  action_noise_clip: 0.5

  critic_loss_params:
    criterion: HuberLoss
    clip_delta: 5.0

  actor_optimizer_params:
    optimizer: Adam
    lr: 0.001
  critic_optimizer_params:
    optimizer: Adam
    lr: 0.001

  actor_scheduler_params:
    scheduler: MultiStepLR
    milestones: [200000, 400000, 600000]  # batches
    gamma: 0.8

  critic_scheduler_params:
    scheduler: MultiStepLR
    milestones: [200000, 400000, 600000]  # batches
    gamma: 0.8

#   actor_grad_clip_params:
#     func: clip_grad_value_
#     clip_value: 1.0

trainer:
  batch_size: 2048             # transitions
  num_workers: 1
  epoch_len: 1000              # batches

  replay_buffer_size: 1000000  # transitions
  start_learning: 10000        # transitions

  save_period: 1000            # epochs
  weights_sync_period: 1       # epochs
  target_update_period: 1      # batches

sampler:
  weights_sync_period: 1

  valid_seeds: [1, 10, 1000, 10000, 42000]

  exploration_params:
    - exploration: GaussNoise
      probability: 0.6
      sigma: 0.2

    - exploration: ParameterSpaceNoise
      probability: 0.3
      target_sigma: 0.2

    - exploration: NoExploration
      probability: 0.1
