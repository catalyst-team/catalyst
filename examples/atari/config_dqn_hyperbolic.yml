args:
  logdir: ./logs/atari-dqn
  expdir: src

  vis: 0
  infer: 0   #  change me
  train: 8   #  change me

db:
  db: MongoDB  # RedisDB or MongoDB
  port: 12000
  prefix: gym-dqn  # TODO: remove

environment:
  environment: AtariEnvWrapper
  env_id: PongNoFrameskip-v4

  history_len: 4
  frame_skip: 4
  reward_scale: 1.0
  step_delay: 0.0

agents:
  critic:
    agent: ConvQCritic

#    state_net_params:  # state -> hidden representation
#      observation_net_params:
#        hiddens: [32]  # first hidden would be taken from state_shape
#        layer_fn: Linear
#        norm_fn: LayerNorm
#        activation_fn: ReLU
#        bias: false
#      main_net_params:
#        hiddens: [32, 32]
#        layer_fn: Linear
#        norm_fn: LayerNorm
#        activation_fn: ReLU
#        bias: false
    value_head_params:  # hidden representation -> ~policy
      in_features: 512  # out features would be taken from action_shape
#      num_atoms: 1

      distribution: categorical
      num_atoms: 51
      values_range: [-4.0, 8.0]
      num_heads: 10
      hyperbolic_constant: 0.01

#      distribution: quantile
#      num_atoms: 51

algorithm:
  algorithm: DQN

  n_step: 5
  gamma: 0.99
  critic_tau: 1.0

#  critic_loss_params:
#    criterion: HuberLoss
#    clip_delta: 1.0

  critic_optimizer_params:
    optimizer: Adam
    lr: 0.001

  critic_grad_clip_params:
    func: clip_grad_value_
    clip_value: 5.0

trainer:
  batch_size: 256               # transitions
  num_workers: 2
  epoch_len: 500                # batches

  replay_buffer_size: 5000000   # transitions
  replay_buffer_mode: memmap    # numpy or memmap
  min_num_transitions: 50000    # transitions

  save_period: 50               # epochs
  weights_sync_period: 1        # epochs
  target_update_period: 250     # batches

sampler:
  weights_sync_period: 1

  exploration_params:
    - exploration: Boltzmann
      probability: 0.5
      temp_init: 1.0
      temp_final: 0.5
      annealing_steps: 100000

    - exploration: EpsilonGreedy
      probability: 0.45
      eps_init: 1.0
      eps_final: 0.5
      annealing_steps: 100000

    - exploration: Greedy
      probability: 0.05