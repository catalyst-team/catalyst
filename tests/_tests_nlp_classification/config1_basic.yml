# Refer to configs/config-description-eng.yml
# for detailed comments on this configuration file

runner_params:
  input_key: ["features", "attention_mask"]

model_params:
  _target_:  BertClassifier
  pretrained_model_name: distilbert-base-uncased
  num_classes: 6

args:
  # where to look for __init__.py file
  expdir: '_tests_nlp_classification'
  # store logs in this subfolder
  baselogdir: './logs/_tests_nlp_classification'

# common settings for all stages
stages:
  # PyTorch loader params
  data_params:
    batch_size: 24
    num_workers: 1
    path_to_data: './tests/_tests_nlp_classification/input'
    train_filename: "train.csv"
    valid_filename: "valid.csv"
    text_field: "text"
    label_field: "label"
    max_sequence_length: 512

  stage_params:
    main_metric: &reduced_metric loss
    minimize_metric: True

  # loss function for the classification task
  criterion_params:
    _target_:  CrossEntropyLoss

  # scheduler controls learning rate during training
  scheduler_params:
    _target_:  ReduceLROnPlateau

  # callbacks serve to calculate loss and metric,
  # update model weights, save checkpoint etc.
  callbacks_params:
    loss:
      _target_:  CriterionCallback
    accuracy:
      _target_:  AccuracyCallback
    optimizer:
      _target_:  OptimizerCallback
      accumulation_steps: 4
    scheduler:
      _target_:  SchedulerCallback
      reduced_metric: *reduced_metric
    saver:
      _target_:  CheckpointCallback
      save_n_best: 3

  # params specific for stage 1 called "train_val"
  train_val:
    # overriding state params and specifying that we train for 3 epochs
    stage_params:
      num_epochs: 3
    # optimizer params are specific only for this stage
    # in principle, we can define another stage with other optim params
    optimizer_params:
      _target_:  Adam
      lr: 0.00005
